# =============================================================================
# MLflow Templates
# =============================================================================
#
# Model comparison and promotion operations.
# Uses mlops-platform-env ConfigMap for MLFLOW_TRACKING_URI.
# =============================================================================

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: mlops
spec:
  templates:
    # =========================================================================
    # Compare CI model to production and promote to staging if better
    # =========================================================================
    - name: compare-to-production-and-promote-to-staging
      inputs:
        parameters:
          - name: model_name
          - name: metric_name
          - name: higher_is_better
          - name: threshold
      outputs:
        parameters:
          - name: promoted
            valueFrom:
              path: /tmp/promoted.txt
          - name: staging_version
            valueFrom:
              path: /tmp/staging_version.txt
          - name: message
            valueFrom:
              path: /tmp/message.txt
        artifacts:
          - name: comparison-report
            path: /tmp/comparison-report.md
            archive:
              none: {}
      script:
        image: python:3.12-slim
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import subprocess
          import sys
          from datetime import datetime
          subprocess.run([sys.executable, "-m", "pip", "install", "-q", "--root-user-action=ignore", "--disable-pip-version-check", "mlflow"], check=True, capture_output=True)

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_NAME = "{{inputs.parameters.model_name}}"
          METRIC = "{{inputs.parameters.metric_name}}"
          HIGHER_BETTER = "{{inputs.parameters.higher_is_better}}" == "true"
          THRESHOLD = float("{{inputs.parameters.threshold}}")
          WORKFLOW_UID = "{{workflow.uid}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          # Helper to write outputs and exit
          def write_outputs_and_exit(promoted, staging_ver, message, logs, exit_code=0):
              with open("/tmp/promoted.txt", "w") as f: f.write("true" if promoted else "false")
              with open("/tmp/staging_version.txt", "w") as f: f.write(staging_ver)
              with open("/tmp/message.txt", "w") as f: f.write(message)
              with open("/tmp/comparison-report.md", "w") as f: f.write(logs)
              sys.exit(exit_code)

          # Build log output
          logs = []
          logs.append("")
          logs.append("##################################################")
          logs.append("üîç MODEL COMPARISON")
          logs.append("##################################################")
          logs.append("")
          logs.append(f"  Model:     {MODEL_NAME}")
          logs.append(f"  Metric:    {METRIC}")
          logs.append(f"  Threshold: {THRESHOLD:.0%}")
          logs.append(f"  Workflow:  {WORKFLOW_UID[:8]}...")
          logs.append("")

          # Find the CI model registered by this workflow
          ci_model_name = f"ci.{MODEL_NAME}"
          staging_model_name = f"staging.{MODEL_NAME}"
          prod_model_name = f"prod.{MODEL_NAME}"

          ci_version = None
          ci_metric = None
          prod_metric = None
          prod_version_num = None

          try:
              ci_versions = client.search_model_versions(f"name='{ci_model_name}'")
              for v in ci_versions:
                  run = client.get_run(v.run_id)
                  if run.data.tags.get("argo_workflow_uid") == WORKFLOW_UID:
                      ci_version = v
                      break

              if not ci_version:
                  logs.append("------------------------------------------")
                  logs.append("‚ùå FAILED: No CI model found for workflow")
                  logs.append("------------------------------------------")
                  logs.append("")
                  logs.append("##################################################")
                  logs.append("##################################################")
                  logs.append("")
                  print("\n".join(logs))
                  write_outputs_and_exit(False, "", "No CI model found", "\n".join(logs), 1)

              ci_run = client.get_run(ci_version.run_id)
              ci_metric = ci_run.data.metrics.get(METRIC)

              # Check if metric exists in CI model
              if ci_metric is None:
                  logs.append(f"  CI Model:  v{ci_version.version}")
                  logs.append("")
                  logs.append("------------------------------------------")
                  logs.append(f"‚ùå FAILED: Metric '{METRIC}' not found in CI model")
                  logs.append("   Check training code logs the correct metric")
                  logs.append("------------------------------------------")
                  logs.append("")
                  logs.append("##################################################")
                  logs.append("##################################################")
                  logs.append("")
                  print("\n".join(logs))
                  write_outputs_and_exit(False, "", f"Metric '{METRIC}' not found in CI model", "\n".join(logs), 1)

              logs.append(f"  CI Model:  v{ci_version.version} ({METRIC}={ci_metric})")

          except Exception as e:
              logs.append("------------------------------------------")
              logs.append(f"‚ùå FAILED: Error finding CI model")
              logs.append(f"   {e}")
              logs.append("------------------------------------------")
              logs.append("")
              logs.append("##################################################")
              logs.append("##################################################")
              logs.append("")
              print("\n".join(logs))
              write_outputs_and_exit(False, "", str(e), "\n".join(logs), 1)

          # Get production champion metric
          try:
              prod_versions = client.search_model_versions(f"name='{prod_model_name}'")
              for v in prod_versions:
                  if "champion" in v.aliases:
                      prod_run = client.get_run(v.run_id)
                      prod_metric = prod_run.data.metrics.get(METRIC)
                      prod_version_num = v.version

                      # Check if metric exists in champion
                      if prod_metric is None:
                          logs.append(f"  Champion:  v{v.version}")
                          logs.append("")
                          logs.append("------------------------------------------")
                          logs.append(f"‚ùå FAILED: Metric '{METRIC}' not found in champion")
                          logs.append("   Champion model may use different metric name")
                          logs.append("------------------------------------------")
                          logs.append("")
                          logs.append("##################################################")
                          logs.append("##################################################")
                          logs.append("")
                          print("\n".join(logs))
                          write_outputs_and_exit(False, "", f"Metric '{METRIC}' not found in champion", "\n".join(logs), 1)

                      logs.append(f"  Champion:  v{v.version} ({METRIC}={prod_metric})")
                      break
          except:
              pass

          if prod_metric is None:
              logs.append("  Champion:  (none - first deployment)")

          logs.append("")

          # Compare
          should_promote = False
          if prod_metric is None:
              should_promote = True
              message = "First model - auto-promoting"
          elif HIGHER_BETTER:
              improvement = (ci_metric - prod_metric) / abs(prod_metric) if prod_metric != 0 else 1.0
              should_promote = improvement >= THRESHOLD
              message = f"Improvement: {improvement:.2%} (threshold: {THRESHOLD:.2%})"
          else:
              improvement = (prod_metric - ci_metric) / abs(prod_metric) if prod_metric != 0 else 1.0
              should_promote = improvement >= THRESHOLD
              message = f"Improvement: {improvement:.2%} (threshold: {THRESHOLD:.2%})"

          logs.append("------------------------------------------")
          if should_promote:
              # Copy to staging registry
              staging_version = mlflow.register_model(
                  f"models:/{ci_model_name}/{ci_version.version}",
                  staging_model_name
              )
              client.set_registered_model_alias(staging_model_name, "candidate", staging_version.version)
              logs.append(f"‚úÖ PROMOTED TO STAGING")
              logs.append(f"   {staging_model_name} v{staging_version.version}")
              logs.append(f"   {message}")
              staging_ver = staging_version.version
          else:
              logs.append(f"‚è∏Ô∏è  NOT PROMOTED")
              logs.append(f"   {message}")
              staging_ver = ""

          logs.append("------------------------------------------")
          logs.append("")
          logs.append("##################################################")
          logs.append("##################################################")
          logs.append("")

          # Print logs and write artifact (same content)
          output = "\n".join(logs)
          print(output)

          with open("/tmp/promoted.txt", "w") as f: f.write("true" if should_promote else "false")
          with open("/tmp/staging_version.txt", "w") as f: f.write(staging_ver)
          with open("/tmp/message.txt", "w") as f: f.write(message)
          with open("/tmp/comparison-report.md", "w") as f: f.write(output)

    # =========================================================================
    # Promote staging model to production
    # =========================================================================
    - name: promote-staging-to-production
      inputs:
        parameters:
          - name: model_name
          - name: staging_version
      outputs:
        parameters:
          - name: prod_version
            valueFrom:
              path: /tmp/prod_version.txt
        artifacts:
          - name: promotion-report
            path: /tmp/promotion-report.md
            archive:
              none: {}
      script:
        image: python:3.12-slim
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import subprocess
          import sys
          subprocess.run([sys.executable, "-m", "pip", "install", "-q", "--root-user-action=ignore", "--disable-pip-version-check", "mlflow"], check=True, capture_output=True)

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_NAME = "{{inputs.parameters.model_name}}"
          STAGING_VERSION = "{{inputs.parameters.staging_version}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          staging_model_name = f"staging.{MODEL_NAME}"
          prod_model_name = f"prod.{MODEL_NAME}"

          logs = []
          logs.append("")
          logs.append("##################################################")
          logs.append("üöÄ PROMOTE TO PRODUCTION")
          logs.append("##################################################")
          logs.append("")
          logs.append(f"  Source:  {staging_model_name} v{STAGING_VERSION}")
          logs.append(f"  Target:  {prod_model_name}")
          logs.append("")

          # Move current champion to previous
          try:
              prod_versions = client.search_model_versions(f"name='{prod_model_name}'")
              for v in prod_versions:
                  if "champion" in v.aliases:
                      client.delete_registered_model_alias(prod_model_name, "champion")
                      client.set_registered_model_alias(prod_model_name, "previous", v.version)
                      logs.append(f"  Previous champion v{v.version} ‚Üí @previous")
                      break
          except Exception as e:
              logs.append(f"  No existing champion to demote")

          # Copy staging to production
          prod_version = mlflow.register_model(
              f"models:/{staging_model_name}/{STAGING_VERSION}",
              prod_model_name
          )

          client.set_registered_model_alias(prod_model_name, "champion", prod_version.version)

          logs.append("")
          logs.append("------------------------------------------")
          logs.append(f"‚úÖ PROMOTED TO PRODUCTION")
          logs.append(f"   {prod_model_name} v{prod_version.version} @champion")
          logs.append("------------------------------------------")
          logs.append("")
          logs.append("##################################################")
          logs.append("##################################################")
          logs.append("")

          # Print logs and write artifact (same content)
          output = "\n".join(logs)
          print(output)

          with open("/tmp/promotion-report.md", "w") as f: f.write(output)
          with open("/tmp/prod_version.txt", "w") as f: f.write(prod_version.version)

    # =========================================================================
    # Tag model with deployment info
    # =========================================================================
    - name: tag-deployment-info
      inputs:
        parameters:
          - name: model_uri
          - name: serving_image
          - name: deployed_by
      script:
        image: python:3.12-slim
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import subprocess
          import sys
          from datetime import datetime
          subprocess.run([sys.executable, "-m", "pip", "install", "-q", "--root-user-action=ignore", "--disable-pip-version-check", "mlflow"], check=True, capture_output=True)

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_URI = "{{inputs.parameters.model_uri}}"
          SERVING_IMAGE = "{{inputs.parameters.serving_image}}"
          DEPLOYED_BY = "{{inputs.parameters.deployed_by}}"
          WORKFLOW_NAME = "{{workflow.name}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          # Parse model URI: models:/prod.wine-classifier/3
          parts = MODEL_URI.replace("models:/", "").split("/")
          model_name = parts[0]
          version = parts[1].replace("@", "")

          print("")
          print("##################################################")
          print("üè∑Ô∏è  TAG DEPLOYMENT INFO")
          print("##################################################")
          print("")
          print(f"  Model:    {model_name} v{version}")
          print(f"  Image:    {SERVING_IMAGE}")
          print(f"  Workflow: {WORKFLOW_NAME}")
          print("")

          try:
              deployed_at = datetime.utcnow().isoformat()
              client.set_model_version_tag(model_name, version, "deployed_at", deployed_at)
              client.set_model_version_tag(model_name, version, "deployed_by", DEPLOYED_BY)
              client.set_model_version_tag(model_name, version, "serving_image", SERVING_IMAGE)
              client.set_model_version_tag(model_name, version, "workflow_name", WORKFLOW_NAME)

              print("------------------------------------------")
              print("‚úÖ TAGGED SUCCESSFULLY")
              print(f"   deployed_at: {deployed_at}")
              print("------------------------------------------")
              print("")
          except Exception as e:
              print("------------------------------------------")
              print(f"‚ö†Ô∏è  TAGGING FAILED: {e}")
              print("------------------------------------------")
              print("")
          print("")
          print("##################################################")
          print("##################################################")
          print("")
