# =============================================================================
# MLflow Templates
# =============================================================================
#
# Model comparison and promotion operations.
# Uses mlops-platform-env ConfigMap for MLFLOW_TRACKING_URI.
# =============================================================================

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: mlops
spec:
  templates:
    # =========================================================================
    # Compare CI model to production and promote to staging if better
    # =========================================================================
    - name: compare-to-production-and-promote-to-staging
      inputs:
        parameters:
          - name: model_name
          - name: metric_name
          - name: higher_is_better
          - name: threshold
      outputs:
        parameters:
          - name: promoted
            valueFrom:
              path: /tmp/promoted.txt
          - name: staging_version
            valueFrom:
              path: /tmp/staging_version.txt
          - name: message
            valueFrom:
              path: /tmp/message.txt
        artifacts:
          - name: comparison-report
            path: /tmp/comparison-report.md
            archive:
              none: {}
      script:
        image: python:3.12-slim
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import subprocess
          import sys
          subprocess.run([sys.executable, "-m", "pip", "install", "-q", "--root-user-action=ignore", "--disable-pip-version-check", "mlflow"], check=True, capture_output=True)

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_NAME = "{{inputs.parameters.model_name}}"
          METRIC = "{{inputs.parameters.metric_name}}"
          HIGHER_BETTER = "{{inputs.parameters.higher_is_better}}" == "true"
          THRESHOLD = float("{{inputs.parameters.threshold}}")
          WORKFLOW_UID = "{{workflow.uid}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          print("")
          print("==========================================")
          print("üîç MODEL COMPARISON")
          print("==========================================")
          print("")
          print(f"  Model:     {MODEL_NAME}")
          print(f"  Metric:    {METRIC}")
          print(f"  Threshold: {THRESHOLD:.0%}")
          print(f"  Workflow:  {WORKFLOW_UID[:8]}...")
          print("")

          # Find the CI model registered by this workflow
          ci_model_name = f"ci.{MODEL_NAME}"
          staging_model_name = f"staging.{MODEL_NAME}"
          prod_model_name = f"prod.{MODEL_NAME}"

          try:
              ci_versions = client.search_model_versions(f"name='{ci_model_name}'")
              ci_version = None
              for v in ci_versions:
                  run = client.get_run(v.run_id)
                  if run.data.tags.get("argo_workflow_uid") == WORKFLOW_UID:
                      ci_version = v
                      break

              if not ci_version:
                  print("------------------------------------------")
                  print("‚ùå RESULT: No CI model found")
                  print("------------------------------------------")
                  print("")
                  with open("/tmp/promoted.txt", "w") as f: f.write("false")
                  with open("/tmp/staging_version.txt", "w") as f: f.write("")
                  with open("/tmp/message.txt", "w") as f: f.write("No CI model found")
                  exit(0)

              ci_run = client.get_run(ci_version.run_id)
              ci_metric = ci_run.data.metrics.get(METRIC)
              print(f"  CI Model:  v{ci_version.version} ({METRIC}={ci_metric})")

          except Exception as e:
              print("------------------------------------------")
              print(f"‚ùå RESULT: Error finding CI model")
              print(f"   {e}")
              print("------------------------------------------")
              print("")
              with open("/tmp/promoted.txt", "w") as f: f.write("false")
              with open("/tmp/staging_version.txt", "w") as f: f.write("")
              with open("/tmp/message.txt", "w") as f: f.write(str(e))
              exit(0)

          # Get production champion metric
          prod_metric = None
          try:
              prod_versions = client.search_model_versions(f"name='{prod_model_name}'")
              for v in prod_versions:
                  if "champion" in v.aliases:
                      prod_run = client.get_run(v.run_id)
                      prod_metric = prod_run.data.metrics.get(METRIC)
                      print(f"  Champion:  v{v.version} ({METRIC}={prod_metric})")
                      break
          except:
              pass

          if prod_metric is None:
              print("  Champion:  (none - first deployment)")

          print("")

          # Compare
          should_promote = False
          if prod_metric is None:
              should_promote = True
              message = "First model - auto-promoting"
          elif HIGHER_BETTER:
              improvement = (ci_metric - prod_metric) / abs(prod_metric) if prod_metric != 0 else 1.0
              should_promote = improvement >= THRESHOLD
              message = f"Improvement: {improvement:.2%} (threshold: {THRESHOLD:.2%})"
          else:
              improvement = (prod_metric - ci_metric) / abs(prod_metric) if prod_metric != 0 else 1.0
              should_promote = improvement >= THRESHOLD
              message = f"Improvement: {improvement:.2%} (threshold: {THRESHOLD:.2%})"

          print("------------------------------------------")
          if should_promote:
              # Copy to staging registry
              staging_version = mlflow.register_model(
                  f"models:/{ci_model_name}/{ci_version.version}",
                  staging_model_name
              )
              client.set_registered_model_alias(staging_model_name, "candidate", staging_version.version)
              print(f"‚úÖ PROMOTED TO STAGING")
              print(f"   {staging_model_name} v{staging_version.version}")
              print(f"   {message}")

              with open("/tmp/promoted.txt", "w") as f: f.write("true")
              with open("/tmp/staging_version.txt", "w") as f: f.write(staging_version.version)
          else:
              print(f"‚è∏Ô∏è  NOT PROMOTED")
              print(f"   {message}")
              with open("/tmp/promoted.txt", "w") as f: f.write("false")
              with open("/tmp/staging_version.txt", "w") as f: f.write("")

          print("------------------------------------------")
          print("")
          print("==========================================")
          print("==========================================")
          print("")

          # Generate comparison report
          from datetime import datetime
          with open("/tmp/comparison-report.md", "w") as f:
              f.write(f"# Model Comparison Report\n\n")
              f.write(f"**Model:** `{MODEL_NAME}`\n")
              f.write(f"**Date:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\n")
              f.write(f"**Status:** {'‚úÖ PROMOTED' if should_promote else '‚è∏Ô∏è NOT PROMOTED'}\n\n")
              f.write(f"## Comparison\n\n")
              f.write(f"| | Version | {METRIC} |\n")
              f.write(f"|---|---------|----------|\n")
              f.write(f"| CI Model | v{ci_version.version} | {ci_metric} |\n")
              if prod_metric is not None:
                  f.write(f"| Champion | v{v.version} | {prod_metric} |\n\n")
              else:
                  f.write(f"| Champion | - | (none) |\n\n")
              f.write(f"**Threshold:** {THRESHOLD:.0%}\n")
              f.write(f"**Result:** {message}\n")

          with open("/tmp/message.txt", "w") as f: f.write(message)

    # =========================================================================
    # Promote staging model to production
    # =========================================================================
    - name: promote-staging-to-production
      inputs:
        parameters:
          - name: model_name
          - name: staging_version
      outputs:
        parameters:
          - name: prod_version
            valueFrom:
              path: /tmp/prod_version.txt
        artifacts:
          - name: promotion-report
            path: /tmp/promotion-report.md
            archive:
              none: {}
      script:
        image: python:3.12-slim
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import subprocess
          import sys
          subprocess.run([sys.executable, "-m", "pip", "install", "-q", "--root-user-action=ignore", "--disable-pip-version-check", "mlflow"], check=True, capture_output=True)

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_NAME = "{{inputs.parameters.model_name}}"
          STAGING_VERSION = "{{inputs.parameters.staging_version}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          staging_model_name = f"staging.{MODEL_NAME}"
          prod_model_name = f"prod.{MODEL_NAME}"

          print("")
          print("==========================================")
          print("üöÄ PROMOTE TO PRODUCTION")
          print("==========================================")
          print("")
          print(f"  Source:  {staging_model_name} v{STAGING_VERSION}")
          print(f"  Target:  {prod_model_name}")
          print("")

          # Move current champion to previous
          try:
              prod_versions = client.search_model_versions(f"name='{prod_model_name}'")
              for v in prod_versions:
                  if "champion" in v.aliases:
                      client.delete_registered_model_alias(prod_model_name, "champion")
                      client.set_registered_model_alias(prod_model_name, "previous", v.version)
                      print(f"  Previous champion v{v.version} ‚Üí @previous")
                      break
          except Exception as e:
              print(f"  No existing champion to demote")

          # Copy staging to production
          prod_version = mlflow.register_model(
              f"models:/{staging_model_name}/{STAGING_VERSION}",
              prod_model_name
          )

          client.set_registered_model_alias(prod_model_name, "champion", prod_version.version)

          print("")
          print("------------------------------------------")
          print(f"‚úÖ PROMOTED TO PRODUCTION")
          print(f"   {prod_model_name} v{prod_version.version} @champion")
          print("------------------------------------------")
          print("")
          print("==========================================")
          print("==========================================")
          print("")

          # Generate promotion report
          from datetime import datetime
          with open("/tmp/promotion-report.md", "w") as f:
              f.write(f"# Production Promotion Report\n\n")
              f.write(f"**Model:** `{MODEL_NAME}`\n")
              f.write(f"**Date:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\n")
              f.write(f"**Status:** ‚úÖ PROMOTED\n\n")
              f.write(f"## Details\n\n")
              f.write(f"| Stage | Version |\n")
              f.write(f"|-------|--------|\n")
              f.write(f"| Source (staging) | v{STAGING_VERSION} |\n")
              f.write(f"| Target (prod) | v{prod_version.version} @champion |\n")

          with open("/tmp/prod_version.txt", "w") as f:
              f.write(prod_version.version)

    # =========================================================================
    # Tag model with deployment info
    # =========================================================================
    - name: tag-deployment-info
      inputs:
        parameters:
          - name: model_uri
          - name: serving_image
          - name: deployed_by
      script:
        image: python:3.12-slim
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import subprocess
          import sys
          from datetime import datetime
          subprocess.run([sys.executable, "-m", "pip", "install", "-q", "--root-user-action=ignore", "--disable-pip-version-check", "mlflow"], check=True, capture_output=True)

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_URI = "{{inputs.parameters.model_uri}}"
          SERVING_IMAGE = "{{inputs.parameters.serving_image}}"
          DEPLOYED_BY = "{{inputs.parameters.deployed_by}}"
          WORKFLOW_NAME = "{{workflow.name}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          # Parse model URI: models:/prod.wine-classifier/3
          parts = MODEL_URI.replace("models:/", "").split("/")
          model_name = parts[0]
          version = parts[1].replace("@", "")

          print("")
          print("==========================================")
          print("üè∑Ô∏è  TAG DEPLOYMENT INFO")
          print("==========================================")
          print("")
          print(f"  Model:    {model_name} v{version}")
          print(f"  Image:    {SERVING_IMAGE}")
          print(f"  Workflow: {WORKFLOW_NAME}")
          print("")

          try:
              deployed_at = datetime.utcnow().isoformat()
              client.set_model_version_tag(model_name, version, "deployed_at", deployed_at)
              client.set_model_version_tag(model_name, version, "deployed_by", DEPLOYED_BY)
              client.set_model_version_tag(model_name, version, "serving_image", SERVING_IMAGE)
              client.set_model_version_tag(model_name, version, "workflow_name", WORKFLOW_NAME)

              print("------------------------------------------")
              print("‚úÖ TAGGED SUCCESSFULLY")
              print(f"   deployed_at: {deployed_at}")
              print("------------------------------------------")
              print("")
          except Exception as e:
              print("------------------------------------------")
              print(f"‚ö†Ô∏è  TAGGING FAILED: {e}")
              print("------------------------------------------")
              print("")
          print("")
          print("==========================================")
          print("==========================================")
          print("")
