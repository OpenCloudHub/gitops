# =============================================================================
# MLflow Templates
# =============================================================================
#
# Model comparison and promotion operations for the MLOps pipeline.
# Implements three-registry pattern: ci.* ‚Üí staging.* ‚Üí prod.*
#
# Templates:
#   compare-to-production-and-promote-to-staging - Compare CI model to champion, promote if better
#   promote-staging-to-production                - Copy staging candidate to production champion
#   tag-deployment-info                          - Add deployment metadata to model version
#
# Registry Pattern:
#   ci.{model}      - Models registered by training jobs (tagged with workflow UID)
#   staging.{model} - Validated candidates (@candidate alias)
#   prod.{model}    - Production models (@champion, @previous aliases)
#
# Uses mlops-platform-env ConfigMap for MLFLOW_TRACKING_URI.
#
# =============================================================================

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: mlops
  annotations:
    workflows.argoproj.io/description: "MLflow model comparison and promotion"
spec:
  templates:
    # =========================================================================
    # Compare CI model to production and promote to staging if better
    # =========================================================================
    - name: compare-to-production-and-promote-to-staging
      inputs:
        parameters:
          - name: model_name
          - name: metric_name
          - name: higher_is_better
          - name: threshold
      outputs:
        parameters:
          - name: promoted
            valueFrom:
              path: /tmp/promoted.txt
          - name: staging_version
            valueFrom:
              path: /tmp/staging_version.txt
          - name: message
            valueFrom:
              path: /tmp/message.txt
        artifacts:
          - name: comparison-report
            path: /tmp/comparison-report.md
            archive:
              none: {}
      script:
        image: ghcr.io/mlflow/mlflow:v3.6.0
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import sys

          import mlflow
          from mlflow.tracking import MlflowClient

          # =================================================================
          # CONFIG
          # =================================================================
          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_NAME = "{{inputs.parameters.model_name}}"
          METRIC = "{{inputs.parameters.metric_name}}"
          HIGHER_BETTER = "{{inputs.parameters.higher_is_better}}" == "true"
          THRESHOLD = float("{{inputs.parameters.threshold}}")
          WORKFLOW_UID = "{{workflow.uid}}"

          ci_model_name = f"ci.{MODEL_NAME}"
          staging_model_name = f"staging.{MODEL_NAME}"
          prod_model_name = f"prod.{MODEL_NAME}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          # =================================================================
          # RESULT STATE (populated by logic, consumed by report)
          # =================================================================
          result = {
              "success": False,
              "promoted": False,
              "error": None,
              "ci_version": None,
              "ci_metric": None,
              "prod_version": None,
              "prod_metric": None,
              "staging_version": None,
              "message": "",
          }

          # =================================================================
          # BUSINESS LOGIC
          # =================================================================
          def find_ci_model():
              """Find CI model registered by this workflow."""
              versions = client.search_model_versions(f"name='{ci_model_name}'")
              for v in versions:
                  run = client.get_run(v.run_id)
                  if run.data.tags.get("argo_workflow_uid") == WORKFLOW_UID:
                      metric = run.data.metrics.get(METRIC)
                      return v.version, metric
              return None, None

          def find_champion():
              """Find current production champion."""
              try:
                  versions = client.search_model_versions(f"name='{prod_model_name}'")
                  for v in versions:
                      if "champion" in v.aliases:
                          run = client.get_run(v.run_id)
                          metric = run.data.metrics.get(METRIC)
                          return v.version, metric
              except:
                  pass
              return None, None

          def should_promote(ci_metric, prod_metric):
              """Determine if CI model should be promoted."""
              if prod_metric is None:
                  return True, "First model - auto-promoting"

              if prod_metric == 0:
                  improvement = 1.0
              elif HIGHER_BETTER:
                  improvement = (ci_metric - prod_metric) / abs(prod_metric)
              else:
                  improvement = (prod_metric - ci_metric) / abs(prod_metric)

              promoted = improvement >= THRESHOLD
              message = f"Improvement: {improvement:.2%} (threshold: {THRESHOLD:.2%})"
              return promoted, message

          def promote_to_staging(ci_version):
              """Copy CI model to staging registry."""
              staging = mlflow.register_model(
                  f"models:/{ci_model_name}/{ci_version}",
                  staging_model_name
              )
              client.set_registered_model_alias(staging_model_name, "candidate", staging.version)
              return staging.version

          # --- Execute logic ---
          try:
              # Find CI model
              result["ci_version"], result["ci_metric"] = find_ci_model()

              if result["ci_version"] is None:
                  result["error"] = "No CI model found for this workflow"
              elif result["ci_metric"] is None:
                  result["error"] = f"Metric '{METRIC}' not found in CI model"
              else:
                  # Find champion
                  result["prod_version"], result["prod_metric"] = find_champion()

                  if result["prod_version"] and result["prod_metric"] is None:
                      result["error"] = f"Metric '{METRIC}' not found in champion"
                  else:
                      # Compare and maybe promote
                      result["promoted"], result["message"] = should_promote(
                          result["ci_metric"], result["prod_metric"]
                      )

                      if result["promoted"]:
                          result["staging_version"] = promote_to_staging(result["ci_version"])

                      result["success"] = True

          except Exception as e:
              result["error"] = str(e)

          # =================================================================
          # REPORT GENERATION
          # =================================================================
          def generate_report():
              lines = [
                  "",
                  "##################################################",
                  "üîç MODEL COMPARISON",
                  "##################################################",
                  "",
                  f"  Model ........ {MODEL_NAME}",
                  f"  Metric ....... {METRIC}",
                  f"  Threshold .... {THRESHOLD:.0%}",
                  f"  Workflow ..... {WORKFLOW_UID[:8]}...",
                  "",
              ]

              # CI model info
              if result["ci_version"]:
                  if result["ci_metric"] is not None:
                      lines.append(f"  CI Model ..... v{result['ci_version']} ({METRIC}={result['ci_metric']:.4f})")
                  else:
                      lines.append(f"  CI Model ..... v{result['ci_version']}")

              # Champion info
              if result["prod_version"]:
                  if result["prod_metric"] is not None:
                      lines.append(f"  Champion ..... v{result['prod_version']} ({METRIC}={result['prod_metric']:.4f})")
                  else:
                      lines.append(f"  Champion ..... v{result['prod_version']}")
              elif result["success"]:
                  lines.append("  Champion ..... (none - first deployment)")

              lines.append("")
              lines.append("--------------------------------------------------")

              # Result
              if result["error"]:
                  lines.append(f"‚ùå FAILED - {result['error']}")
              elif result["promoted"]:
                  lines.append("‚úÖ PROMOTED TO STAGING")
                  lines.append(f"   {staging_model_name} v{result['staging_version']} @candidate")
                  lines.append(f"   {result['message']}")
              else:
                  lines.append("‚è∏Ô∏è  NOT PROMOTED")
                  lines.append(f"   {result['message']}")

              lines.extend(["--------------------------------------------------", "", "##################################################", ""])
              return "\n".join(lines)

          # =================================================================
          # OUTPUT
          # =================================================================
          report = generate_report()
          print(report)

          with open("/tmp/comparison-report.md", "w") as f:
              f.write(report)
          with open("/tmp/promoted.txt", "w") as f:
              f.write("true" if result["promoted"] else "false")
          with open("/tmp/staging_version.txt", "w") as f:
              f.write(result["staging_version"] or "")
          with open("/tmp/message.txt", "w") as f:
              f.write(result["message"] or result["error"] or "")

          if result["error"]:
              sys.exit(1)

    # =========================================================================
    # Promote staging model to production
    # =========================================================================
    - name: promote-staging-to-production
      inputs:
        parameters:
          - name: model_name
          - name: staging_version
      outputs:
        parameters:
          - name: prod_version
            valueFrom:
              path: /tmp/prod_version.txt
        artifacts:
          - name: promotion-report
            path: /tmp/promotion-report.md
            archive:
              none: {}
      script:
        image: ghcr.io/mlflow/mlflow:v3.6.0
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import sys

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_NAME = "{{inputs.parameters.model_name}}"
          STAGING_VERSION = "{{inputs.parameters.staging_version}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          staging_model_name = f"staging.{MODEL_NAME}"
          prod_model_name = f"prod.{MODEL_NAME}"

          logs = []
          logs.append("")
          logs.append("##################################################")
          logs.append("üöÄ PROMOTE TO PRODUCTION")
          logs.append("##################################################")
          logs.append("")
          logs.append(f"  Source ....... {staging_model_name} v{STAGING_VERSION}")
          logs.append(f"  Target ....... {prod_model_name}")
          logs.append("")

          try:
              prod_versions = client.search_model_versions(f"name='{prod_model_name}'")
              for v in prod_versions:
                  if "champion" in v.aliases:
                      client.delete_registered_model_alias(prod_model_name, "champion")
                      client.set_registered_model_alias(prod_model_name, "previous", v.version)
                      logs.append(f"  Demoted ...... v{v.version} ‚Üí @previous")
                      break
          except Exception as e:
              logs.append(f"  Demoted ...... (no existing champion)")

          prod_version = mlflow.register_model(
              f"models:/{staging_model_name}/{STAGING_VERSION}",
              prod_model_name
          )

          client.set_registered_model_alias(prod_model_name, "champion", prod_version.version)

          logs.append("")
          logs.append("--------------------------------------------------")
          logs.append(f"‚úÖ PROMOTED TO PRODUCTION")
          logs.append(f"   {prod_model_name} v{prod_version.version} @champion")
          logs.append("--------------------------------------------------")
          logs.append("")
          logs.append("##################################################")
          logs.append("")

          output = "\n".join(logs)
          print(output)

          with open("/tmp/promotion-report.md", "w") as f: f.write(output)
          with open("/tmp/prod_version.txt", "w") as f: f.write(prod_version.version)

    # =========================================================================
    # Tag model with deployment info
    # =========================================================================
    - name: tag-deployment-info
      inputs:
        parameters:
          - name: model_uri
          - name: serving_image
          - name: deployed_by
      outputs:
        artifacts:
          - name: tagging-report
            path: /tmp/tagging-report.md
            archive:
              none: {}
      script:
        image: ghcr.io/mlflow/mlflow:v3.6.0
        envFrom:
          - configMapRef:
              name: mlops-platform-env
        command: [python]
        source: |
          import os
          import sys
          from datetime import datetime

          import mlflow
          from mlflow.tracking import MlflowClient

          MLFLOW_URI = os.environ["MLFLOW_TRACKING_URI"]
          MODEL_URI = "{{inputs.parameters.model_uri}}"
          SERVING_IMAGE = "{{inputs.parameters.serving_image}}"
          DEPLOYED_BY = "{{inputs.parameters.deployed_by}}"
          WORKFLOW_NAME = "{{workflow.name}}"

          mlflow.set_tracking_uri(MLFLOW_URI)
          client = MlflowClient()

          parts = MODEL_URI.replace("models:/", "").split("/")
          model_name = parts[0]
          version = parts[1].replace("@", "")

          logs = []
          logs.append("")
          logs.append("##################################################")
          logs.append("üè∑Ô∏è  TAG DEPLOYMENT INFO")
          logs.append("##################################################")
          logs.append("")
          logs.append(f"  Model ........ {model_name} v{version}")
          logs.append(f"  Image ........ {SERVING_IMAGE}")
          logs.append(f"  Workflow ..... {WORKFLOW_NAME}")
          logs.append("")

          try:
              deployed_at = datetime.utcnow().isoformat()
              client.set_model_version_tag(model_name, version, "deployed_at", deployed_at)
              client.set_model_version_tag(model_name, version, "deployed_by", DEPLOYED_BY)
              client.set_model_version_tag(model_name, version, "serving_image", SERVING_IMAGE)
              client.set_model_version_tag(model_name, version, "workflow_name", WORKFLOW_NAME)

              logs.append("--------------------------------------------------")
              logs.append("‚úÖ TAGGED SUCCESSFULLY")
              logs.append(f"   deployed_at: {deployed_at}")
              logs.append("--------------------------------------------------")
          except Exception as e:
              logs.append("--------------------------------------------------")
              logs.append(f"‚ö†Ô∏è  TAGGING FAILED: {e}")
              logs.append("--------------------------------------------------")

          logs.append("")
          logs.append("##################################################")
          logs.append("")

          output = "\n".join(logs)
          print(output)

          with open("/tmp/tagging-report.md", "w") as f: f.write(output)
