# =============================================================================
# MLflow Templates
# =============================================================================
#
# Templates for MLflow model registry operations.
# =============================================================================

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: mlops
  annotations:
    workflows.argoproj.io/description: "MLflow model registry operations"
spec:
  templates:
    # =========================================================================
    # COMPARE TO PRODUCTION AND PROMOTE TO STAGING
    # =========================================================================
    - name: compare-to-production-and-promote-to-staging
      inputs:
        parameters:
          - name: model_name
          - name: metric_name
          - name: higher_is_better
          - name: threshold
          - name: mlflow_tracking_uri
      outputs:
        parameters:
          - name: ci_version
            valueFrom:
              path: /tmp/ci_version.txt
          - name: staging_version
            valueFrom:
              path: /tmp/staging_version.txt
          - name: ci_run_id
            valueFrom:
              path: /tmp/ci_run_id.txt
          - name: promoted
            valueFrom:
              path: /tmp/promoted.txt
          - name: reason
            valueFrom:
              path: /tmp/reason.txt
        artifacts:
          - name: comparison-report
            path: /tmp/comparison-report.json
            archive:
              none: {}
      script:
        image: ghcr.io/mlflow/mlflow:v2.18.0
        command: [python]
        source: |
          from mlflow import MlflowClient
          from mlflow.exceptions import MlflowException
          import mlflow
          from datetime import datetime
          import sys
          import json

          def write_outputs(promoted, reason, ci_version="", staging_version="", ci_run_id="", report=None):
              """Write all required output files"""
              with open("/tmp/promoted.txt", "w") as f:
                  f.write("true" if promoted else "false")
              with open("/tmp/reason.txt", "w") as f:
                  f.write(reason)
              with open("/tmp/ci_version.txt", "w") as f:
                  f.write(str(ci_version))
              with open("/tmp/staging_version.txt", "w") as f:
                  f.write(str(staging_version))
              with open("/tmp/ci_run_id.txt", "w") as f:
                  f.write(ci_run_id)
              if report:
                  with open("/tmp/comparison-report.json", "w") as f:
                      json.dump(report, f, indent=2)

          client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
          mlflow.set_tracking_uri("{{inputs.parameters.mlflow_tracking_uri}}")

          model_name = "{{inputs.parameters.model_name}}"
          metric_name = "{{inputs.parameters.metric_name}}"
          higher_is_better = "{{inputs.parameters.higher_is_better}}" == "true"
          threshold = float("{{inputs.parameters.threshold}}")
          workflow_uid = "{{workflow.uid}}"

          print(f"ðŸ” Comparing {model_name}")
          print(f"   Workflow: {workflow_uid}")
          print(f"   Metric: {metric_name} ({'higher' if higher_is_better else 'lower'} is better)")
          print(f"   Threshold: {threshold}")

          report = {
              "workflow_uid": workflow_uid,
              "model_name": model_name,
              "metric_name": metric_name,
              "higher_is_better": higher_is_better,
              "threshold": threshold,
              "timestamp": datetime.utcnow().isoformat()
          }

          # Get most recent run from this workflow
          print(f"\nðŸ“Š Getting most recent run from workflow {workflow_uid}...")
          runs_df = mlflow.search_runs(
              search_all_experiments=True,
              filter_string=f"tags.argo_workflow_uid = '{workflow_uid}'",
              max_results=1,
              order_by=["start_time DESC"]
          )

          if runs_df.empty:
              reason = f"No runs found with tag argo_workflow_uid='{workflow_uid}'"
              print(f"âŒ {reason}")
              write_outputs(False, reason, report=report)
              sys.exit(1)

          run_id = runs_df.iloc[0]['run_id']
          print(f"   Found run: {run_id[:8]}...")

          # Get CI model from this run
          print(f"\nðŸ“¦ Looking for CI model from run {run_id[:8]}...")
          ci_versions = client.search_model_versions(
              filter_string=f"name='ci.{model_name}' and run_id='{run_id}'"
          )

          if not ci_versions:
              reason = f"No CI model found from run {run_id}"
              print(f"âŒ {reason}")
              write_outputs(False, reason, report=report)
              sys.exit(1)

          ci_model = ci_versions[0]
          ci_version = ci_model.version
          ci_run_id = ci_model.run_id
          print(f"âœ… Found CI model: v{ci_version}")

          # Get CI metrics
          ci_run = client.get_run(ci_run_id)
          if metric_name not in ci_run.data.metrics:
              reason = f"Metric '{metric_name}' not found in CI model run"
              print(f"âŒ {reason}")
              write_outputs(False, reason, ci_version=ci_version, ci_run_id=ci_run_id, report=report)
              sys.exit(1)

          ci_value = ci_run.data.metrics[metric_name]
          print(f"   CI {metric_name}: {ci_value}")
          report["ci_metric_value"] = ci_value

          # Compare to champion
          print(f"\nðŸ† Looking for champion...")
          should_promote = False

          try:
              champion = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
              champ_run = client.get_run(champion.run_id)

              if metric_name not in champ_run.data.metrics:
                  reason = f"Champion missing metric '{metric_name}'"
                  print(f"âŒ {reason}")
                  write_outputs(False, reason, ci_version=ci_version, ci_run_id=ci_run_id, report=report)
                  sys.exit(1)

              champion_value = champ_run.data.metrics[metric_name]
              print(f"   Champion {metric_name}: {champion_value}")
              report["champion_metric_value"] = champion_value

              if higher_is_better:
                  improvement = ci_value - champion_value
              else:
                  improvement = champion_value - ci_value

              improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0
              print(f"   Improvement: {improvement:.6f} ({improvement_pct:.2f}%)")
              report["improvement"] = improvement

              if improvement >= threshold:
                  reason = f"Improvement {improvement:.6f} exceeds threshold {threshold}"
                  should_promote = True
              else:
                  reason = f"Insufficient improvement: {improvement:.6f} < {threshold}"

          except MlflowException:
              reason = "No champion exists, promoting first model"
              print(f"   {reason}")
              should_promote = True

          if not should_promote:
              print(f"âŒ {reason}")
              write_outputs(False, reason, ci_version=ci_version, ci_run_id=ci_run_id, report=report)
              sys.exit(0)

          # Promote to staging
          print(f"\nðŸ“¦ Copying ci.{model_name}/v{ci_version} â†’ staging.{model_name}")
          staging_mv = client.copy_model_version(
              f"models:/ci.{model_name}/{ci_version}",
              f"staging.{model_name}"
          )
          print(f"âœ… Created staging.{model_name}/v{staging_mv.version}")

          write_outputs(
              True, reason,
              ci_version=ci_version,
              staging_version=staging_mv.version,
              ci_run_id=ci_run_id,
              report=report
          )
          print(f"\nâœ… Promoted to staging")

    # =========================================================================
    # PROMOTE STAGING TO PRODUCTION
    # =========================================================================
    - name: promote-staging-to-production
      inputs:
        parameters:
          - name: model_name
          - name: staging_version
          - name: mlflow_tracking_uri
      outputs:
        parameters:
          - name: prod_version
            valueFrom:
              path: /tmp/prod_version.txt
      script:
        image: ghcr.io/mlflow/mlflow:v2.18.0
        command: [python]
        source: |
          from mlflow import MlflowClient

          client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
          model_name = "{{inputs.parameters.model_name}}"
          staging_version = "{{inputs.parameters.staging_version}}"

          print(f"ðŸš€ Promoting to production: {model_name}")

          prod_mv = client.copy_model_version(
              f"models:/staging.{model_name}/{staging_version}",
              f"prod.{model_name}"
          )
          print(f"âœ… Created prod.{model_name}/v{prod_mv.version}")

          # Move old champion to previous
          try:
              old_champ = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
              print(f"   Moving old champion v{old_champ.version} â†’ @previous")
              client.set_registered_model_alias(f"prod.{model_name}", "previous", old_champ.version)
          except:
              print("   No previous champion")

          client.set_registered_model_alias(f"prod.{model_name}", "champion", prod_mv.version)
          print(f"âœ… Set prod.{model_name}@champion â†’ v{prod_mv.version}")

          with open("/tmp/prod_version.txt", "w") as f:
              f.write(prod_mv.version)

    # =========================================================================
    # TAG DEPLOYMENT INFO
    # =========================================================================
    - name: tag-deployment-info
      inputs:
        parameters:
          - name: model_uri
          - name: serving_image
          - name: deployed_by
            default: "manual"
          - name: mlflow_tracking_uri
            default: "http://mlflow.mlops.svc.cluster.local:80"
      script:
        image: ghcr.io/mlflow/mlflow:v2.18.0
        command: [python]
        source: |
          from mlflow import MlflowClient
          from datetime import datetime
          import re
          import os

          mlflow_uri = "{{inputs.parameters.mlflow_tracking_uri}}"
          model_uri = "{{inputs.parameters.model_uri}}".strip()
          serving_image = "{{inputs.parameters.serving_image}}".strip()
          deployed_by = "{{inputs.parameters.deployed_by}}".strip()

          client = MlflowClient(mlflow_uri)

          # Parse: models:/prod.wine-classifier/1
          match = re.match(r'models:/([^/]+)/(\d+)', model_uri)
          if not match:
              print(f"âŒ Invalid model URI: '{model_uri}'")
              exit(1)

          full_name = match.group(1)
          version = match.group(2)

          print(f"ðŸ·ï¸  Tagging deployment: {full_name}/v{version}")

          tags = {
              "deployment_status": "deployed",
              "deployed_at": datetime.utcnow().isoformat(),
              "deployed_by": deployed_by,
              "serving_image": serving_image,
              "kubernetes_namespace": "ai",
              "argo_workflow_uid": "{{workflow.uid}}",
              "argo_workflow_name": "{{workflow.name}}"
          }

          for key, value in tags.items():
              client.set_model_version_tag(full_name, version, key, value)

          print(f"âœ… Tagged: {full_name}/v{version}")
