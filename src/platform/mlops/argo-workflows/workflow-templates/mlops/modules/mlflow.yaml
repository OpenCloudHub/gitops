apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: mlops
  annotations:
    workflows.argoproj.io/description: 'Templates to interact with MLflow.'
spec:
  templates:
  - name: compare-to-production-and-promote-to-staging
    inputs:
      parameters:
      - name: model_name
      - name: metric_name
      - name: higher_is_better
      - name: threshold
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: ci_version
        valueFrom:
          path: /tmp/ci_version.txt
      - name: staging_version
        valueFrom:
          path: /tmp/staging_version.txt
      - name: ci_run_id
        valueFrom:
          path: /tmp/ci_run_id.txt
      - name: promoted
        valueFrom:
          path: /tmp/promoted.txt
      - name: reason
        valueFrom:
          path: /tmp/reason.txt
      artifacts:
      - name: comparison-report
        path: /tmp/comparison-report.json
        archive:
          none: {}
      - name: comparison-summary
        path: /tmp/comparison-summary.md
        archive:
          none: {}
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient
        from mlflow.exceptions import MlflowException
        import mlflow
        from datetime import datetime
        import sys
        import json

        def write_failure_outputs(reason, report):
            """Write all required output files for failure cases"""
            report["promoted"] = False
            report["reason"] = reason

            with open("/tmp/promoted.txt", "w") as f:
                f.write("false")
            with open("/tmp/reason.txt", "w") as f:
                f.write(reason)
            with open("/tmp/comparison-report.json", "w") as f:
                json.dump(report, f, indent=2)
            with open("/tmp/comparison-summary.md", "w") as f:
                f.write(f"# âŒ Comparison Failed\n\n**Reason:** {reason}\n")

            # Write empty placeholders for required outputs
            with open("/tmp/ci_version.txt", "w") as f:
                f.write("")
            with open("/tmp/staging_version.txt", "w") as f:
                f.write("")
            with open("/tmp/ci_run_id.txt", "w") as f:
                f.write("")

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        mlflow.set_tracking_uri("{{inputs.parameters.mlflow_tracking_uri}}")

        model_name = "{{inputs.parameters.model_name}}"
        metric_name = "{{inputs.parameters.metric_name}}"
        higher_is_better = "{{inputs.parameters.higher_is_better}}" == "true"
        threshold = float("{{inputs.parameters.threshold}}")
        workflow_uid = "{{workflow.uid}}"
        workflow_name = "{{workflow.name}}"

        print(f"ðŸ” Comparing {model_name}")
        print(f"   Workflow: {workflow_uid}")
        print(f"   Metric: {metric_name} ({'higher' if higher_is_better else 'lower'} is better)")
        print(f"   Threshold: {threshold}")

        # Initialize comparison report
        report = {
            "workflow_uid": workflow_uid,
            "workflow_name": workflow_name,
            "model_name": model_name,
            "metric_name": metric_name,
            "higher_is_better": higher_is_better,
            "threshold": threshold,
            "timestamp": datetime.utcnow().isoformat()
        }

        # Get the most recent run from this workflow
        print(f"\nðŸ“Š Getting most recent run from workflow {workflow_uid}...")
        runs_df = mlflow.search_runs(
            search_all_experiments=True,
            filter_string=f"tags.argo_workflow_uid = '{workflow_uid}'",
            max_results=1,
            order_by=["start_time DESC"]
        )

        if runs_df.empty:
            reason = f"No runs found with tag argo_workflow_uid='{workflow_uid}'"
            print(f"âŒ {reason}")
            write_failure_outputs(reason, report)
            sys.exit(1)

        run_id = runs_df.iloc[0]['run_id']
        print(f"   Found run: {run_id[:8]}...")
        report["ci_run_id"] = run_id

        # Get CI model from this run
        print(f"\nðŸ“¦ Looking for CI model from run {run_id[:8]}...")
        ci_versions = client.search_model_versions(
            filter_string=f"name='ci.{model_name}' and run_id='{run_id}'"
        )

        if not ci_versions:
            reason = f"No CI model found from run {run_id}"
            print(f"âŒ {reason}")
            write_failure_outputs(reason, report)
            sys.exit(1)

        ci_model = ci_versions[0]
        ci_version = ci_model.version
        ci_run_id = ci_model.run_id
        print(f"âœ… Found CI model: v{ci_version}")

        report["ci_version"] = str(ci_version)
        report["ci_model_uri"] = f"models:/ci.{model_name}/{ci_version}"

        # Tag the CI model
        print(f"\nðŸ·ï¸  Tagging CI model...")
        ci_tags = {
            "argo_workflow_uid": workflow_uid,
            "argo_workflow_name": workflow_name,
            "tagged_by": "ci_pipeline",
            "tagged_at": datetime.utcnow().isoformat()
        }

        for key, value in ci_tags.items():
            client.set_model_version_tag(f"ci.{model_name}", ci_version, key, value)

        # Get CI metrics
        print(f"\nðŸ“Š Getting metrics from CI model...")
        ci_run = client.get_run(ci_run_id)
        if metric_name not in ci_run.data.metrics:
            reason = f"Metric '{metric_name}' not found in CI model run"
            print(f"âŒ {reason}")
            client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "rejected")
            client.set_model_version_tag(f"ci.{model_name}", ci_version, "rejection_reason", reason)
            write_failure_outputs(reason, report)
            sys.exit(1)

        ci_value = ci_run.data.metrics[metric_name]
        print(f"   CI {metric_name}: {ci_value}")
        report["ci_metric_value"] = ci_value

        # Try to get champion
        print(f"\nðŸ† Looking for champion...")
        should_promote = False
        champion_version = None
        champion_value = None
        improvement = None

        # Do the comparison
        try:
            # Get the current champion
            champion = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
            champion_version = champion.version
            print(f"   Found champion: v{champion_version}")
            report["champion_version"] = str(champion_version)
            report["champion_model_uri"] = f"models:/prod.{model_name}/{champion_version}"

            champ_run = client.get_run(champion.run_id)
            # Check if metrics are in champion model
            if metric_name not in champ_run.data.metrics:
                reason = f"Champion v{champion_version} missing metric '{metric_name}' - cannot compare. Investigate champion model before promoting."
                print(f"âŒ {reason}")
                client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "blocked")
                client.set_model_version_tag(f"ci.{model_name}", ci_version, "block_reason", reason)
                report["champion_metric_value"] = None
                report["improvement"] = "N/A"
                report["reason"] = reason
                write_failure_outputs(reason, report)
                sys.exit(1)
            else:
                champion_value = champ_run.data.metrics[metric_name]
                print(f"   Champion {metric_name}: {champion_value}")
                report["champion_metric_value"] = champion_value

                if higher_is_better:
                    improvement = ci_value - champion_value
                    improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0
                else:
                    improvement = champion_value - ci_value
                    improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0

                print(f"   Improvement: {improvement:.6f} ({improvement_pct:.2f}%)")
                report["improvement"] = improvement
                report["improvement_pct"] = improvement_pct

                if improvement >= threshold:
                    reason = f"CI model outperforms champion by {improvement:.6f} ({improvement_pct:.2f}%), exceeds threshold {threshold}"
                    print(f"âœ… {reason}")
                    should_promote = True
                    report["reason"] = reason
                else:
                    reason = f"Insufficient improvement: {improvement:.6f} ({improvement_pct:.2f}%) < threshold {threshold}"
                    print(f"âŒ {reason}")
                    client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "rejected")
                    client.set_model_version_tag(f"ci.{model_name}", ci_version, "rejection_reason", reason)

                    report["promoted"] = False
                    report["reason"] = reason

                    # Write all outputs including required empty ones
                    with open("/tmp/promoted.txt", "w") as f:
                        f.write("false")
                    with open("/tmp/reason.txt", "w") as f:
                        f.write(reason)
                    with open("/tmp/comparison-report.json", "w") as f:
                        json.dump(report, f, indent=2)
                    with open("/tmp/ci_version.txt", "w") as f:
                        f.write(str(ci_version))
                    with open("/tmp/staging_version.txt", "w") as f:
                        f.write("")
                    with open("/tmp/ci_run_id.txt", "w") as f:
                        f.write(ci_run_id)

                    # Create markdown summary
                    with open("/tmp/comparison-summary.md", "w") as f:
                        f.write(f"# âŒ Model Not Promoted\n\n")
                        f.write(f"## Comparison Results\n\n")
                        f.write(f"- **Model:** `{model_name}`\n")
                        f.write(f"- **Metric:** `{metric_name}` ({'higher' if higher_is_better else 'lower'} is better)\n")
                        f.write(f"- **CI Model:** `v{ci_version}` = **{ci_value:.6f}**\n")
                        f.write(f"- **Champion:** `v{champion_version}` = **{champion_value:.6f}**\n")
                        f.write(f"- **Improvement:** {improvement:.6f} ({improvement_pct:.2f}%)\n")
                        f.write(f"- **Required Threshold:** {threshold}\n\n")
                        f.write(f"## Decision\n\n")
                        f.write(f"â›” **Not promoted:** {reason}\n")

                    print(f"âœ… Comparison complete - model not promoted (below threshold)")

        except MlflowException as e:
            # No champion exists - this is expected for first deployment
            reason = "No champion exists, promoting first model"
            print(f"   {reason}")
            should_promote = True
            improvement = "N/A (first deployment)"
            report["champion_version"] = None
            report["champion_metric_value"] = None
            report["improvement"] = improvement
            report["reason"] = reason

        if not should_promote:
            sys.exit(0)

        # Copy to staging
        print(f"\nðŸ“¦ Copying ci.{model_name}/v{ci_version} â†’ staging.{model_name}")
        staging_mv = client.copy_model_version(
            f"models:/ci.{model_name}/{ci_version}",
            f"staging.{model_name}"
        )

        print(f"âœ… Created staging.{model_name}/v{staging_mv.version}")
        report["staging_version"] = str(staging_mv.version)
        report["staging_model_uri"] = f"models:/staging.{model_name}/{staging_mv.version}"
        report["promoted"] = True

        # Write all outputs
        with open("/tmp/ci_version.txt", "w") as f:
            f.write(str(ci_version))
        with open("/tmp/staging_version.txt", "w") as f:
            f.write(str(staging_mv.version))
        with open("/tmp/ci_run_id.txt", "w") as f:
            f.write(ci_run_id)
        with open("/tmp/promoted.txt", "w") as f:
            f.write("true")
        with open("/tmp/reason.txt", "w") as f:
            f.write(report["reason"])
        with open("/tmp/comparison-report.json", "w") as f:
            json.dump(report, f, indent=2)

        # Create markdown summary for success
        with open("/tmp/comparison-summary.md", "w") as f:
            f.write(f"# âœ… Model Promoted to Staging\n\n")
            f.write(f"## Comparison Results\n\n")
            f.write(f"- **Model:** `{model_name}`\n")
            f.write(f"- **Metric:** `{metric_name}` ({'higher' if higher_is_better else 'lower'} is better)\n")
            f.write(f"- **CI Model:** `v{ci_version}` = **{ci_value:.6f}**\n")

            if champion_version:
                f.write(f"- **Champion:** `v{champion_version}` = **{champion_value:.6f}**\n")
                if isinstance(improvement, float):
                    f.write(f"- **Improvement:** {improvement:.6f} ({report.get('improvement_pct', 0):.2f}%)\n")
                else:
                    f.write(f"- **Improvement:** {improvement}\n")
            else:
                f.write(f"- **Champion:** None (first deployment)\n")

            f.write(f"- **Required Threshold:** {threshold}\n\n")
            f.write(f"## Decision\n\n")
            f.write(f"âœ… **Promoted:** {report['reason']}\n\n")
            f.write(f"## Next Steps\n\n")
            f.write(f"- Model copied to staging: `staging.{model_name}/v{staging_mv.version}`\n")
            f.write(f"- Awaiting approval for production deployment\n")

        print(f"\nâœ… Comparison complete - model promoted to staging")

  - name: promote-staging-to-production
    inputs:
      parameters:
      - name: model_name
      - name: staging_version
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: prod_version
        valueFrom:
          path: /tmp/prod_version.txt
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        model_name = "{{inputs.parameters.model_name}}"
        staging_version = "{{inputs.parameters.staging_version}}"

        print(f"ðŸš€ Promoting to production: {model_name}")

        prod_mv = client.copy_model_version(
            f"models:/staging.{model_name}/{staging_version}",
            f"prod.{model_name}"
        )

        print(f"âœ… Created prod.{model_name}/v{prod_mv.version}")

        try:
            old_champ = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
            print(f"   Moving old champion v{old_champ.version} â†’ @previous")
            client.set_registered_model_alias(f"prod.{model_name}", "previous", old_champ.version)
        except:
            print("   No previous champion")

        client.set_registered_model_alias(f"prod.{model_name}", "champion", prod_mv.version)
        print(f"âœ… Set prod.{model_name}@champion â†’ v{prod_mv.version}")

        with open("/tmp/prod_version.txt", "w") as f:
            f.write(prod_mv.version)

  - name: tag-deployment-info
    inputs:
      parameters:
      - name: model_uri
      - name: serving_image
      - name: deployed_by
        value: "manual"
      - name: mlflow_tracking_uri
        value: "http://mlflow.mlops.svc.cluster.local:80"
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      env:
      - name: MODEL_URI
        value: "{{inputs.parameters.model_uri}}"
      - name: SERVING_IMAGE
        value: "{{inputs.parameters.serving_image}}"
      - name: DEPLOYED_BY
        value: "{{inputs.parameters.deployed_by}}"
      - name: MLFLOW_TRACKING_URI
        value: "{{inputs.parameters.mlflow_tracking_uri}}"
      - name: WORKFLOW_UID
        value: "{{workflow.uid}}"
      - name: WORKFLOW_NAME
        value: "{{workflow.name}}"
      source: |
        from mlflow import MlflowClient
        from datetime import datetime
        import re
        import os

        client = MlflowClient(os.environ["MLFLOW_TRACKING_URI"])
        model_uri = os.environ["MODEL_URI"].strip()

        # Parse: models:/prod.wine-classifier/1
        match = re.match(r'models:/([^.]+)\.([^/]+)/(\d+)', model_uri)
        if not match:
            print(f"âŒ Invalid model URI format: '{model_uri}'")  # â† Add quotes to see whitespace
            exit(1)

        registry_prefix = match.group(1)
        model_name = match.group(2)
        version = match.group(3)
        full_name = f"{registry_prefix}.{model_name}"

        print(f"ðŸ·ï¸  Tagging deployment in MLflow")
        print(f"   Model: {full_name}/v{version}")

        tags = {
            "deployment_status": "deployed",
            "deployed_at": datetime.utcnow().isoformat(),
            "deployed_by": os.environ["DEPLOYED_BY"].strip(),
            "serving_image": os.environ["SERVING_IMAGE"].strip(),
            "kubernetes_namespace": "ai",
            "argo_workflow_uid": os.environ["WORKFLOW_UID"].strip(),
            "argo_workflow_name": os.environ["WORKFLOW_NAME"].strip()
        }

        for key, value in tags.items():
            client.set_model_version_tag(full_name, version, key, value)

        print(f"âœ… Tagged: {full_name}/v{version}")
