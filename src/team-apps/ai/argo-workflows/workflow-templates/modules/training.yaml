# modules/training.yaml
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: training-templates
  namespace: ai
spec:
  serviceAccountName: workflow-executor

  templates:
  - name: submit-ray-job
    inputs:
      parameters:
      - name: model_name
      - name: experiment_name
      - name: training_image
      - name: training_entrypoint
      - name: training_args
      - name: cpu
      - name: memory
      - name: replicas
      - name: gpu
      - name: mlflow_tracking_uri
      - name: dvc_data_version
    outputs:
      parameters:
      - name: job-name
        valueFrom:
          path: /tmp/job-name.txt
    script:
      image: bitnami/kubectl:latest
      command: [sh]
      source: |
        set -e

        GPU="{{inputs.parameters.gpu}}"

        GPU_RESOURCES=""
        GPU_NODE_SELECTOR=""
        if [ "$GPU" != "0" ]; then
          GPU_RESOURCES="nvidia.com/gpu: \"$GPU\""
          GPU_NODE_SELECTOR="nodeSelector:
          node.opencloudhub.org/gpu: \"true\""
        fi

        RUN_LABEL="wf-$(date +%s)-${RANDOM}"

        # Combine training args with data version
        TRAINING_ARGS="{{inputs.parameters.training_args}}"
        DVC_VERSION="{{inputs.parameters.dvc_data_version}}"

        # Add --data-version if not empty
        if [ -n "$DVC_VERSION" ]; then
          FULL_ARGS="${TRAINING_ARGS} --data-version ${DVC_VERSION}"
        else
          FULL_ARGS="${TRAINING_ARGS}"
        fi

        echo "Creating RayJob with label: workflow-run=${RUN_LABEL}"
        echo "Training command: {{inputs.parameters.training_entrypoint}} ${FULL_ARGS}"

        cat <<EOF | kubectl create -f -
        apiVersion: ray.io/v1
        kind: RayJob
        metadata:
          generateName: {{inputs.parameters.model_name}}-training-
          namespace: ai
          labels:
            workflow-run: "${RUN_LABEL}"
            model: "{{inputs.parameters.model_name}}"
        spec:
          entrypoint: {{inputs.parameters.training_entrypoint}} ${FULL_ARGS}
          shutdownAfterJobFinishes: true
          ttlSecondsAfterFinished: 300
          rayClusterSpec:
            rayVersion: "2.48.0"
            headGroupSpec:
              serviceType: ClusterIP
              template:
                spec:
                  ${GPU_NODE_SELECTOR}
                  containers:
                  - name: ray-head
                    image: {{inputs.parameters.training_image}}
                    resources:
                      requests:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                      limits:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                        ${GPU_RESOURCES}
                    env:
                    - name: MLFLOW_TRACKING_URI
                      value: {{inputs.parameters.mlflow_tracking_uri}}
                    - name: MLFLOW_EXPERIMENT_NAME
                      value: {{inputs.parameters.experiment_name}}
                    - name: MLFLOW_REGISTERED_MODEL_NAME
                      value: "ci.{{inputs.parameters.model_name}}"
                    - name: DVC_DATA_VERSION
                      value: {{inputs.parameters.dvc_data_version}}
                    - name: DOCKER_IMAGE_TAG
                      value: {{inputs.parameters.training_image}}
                    - name: ARGO_WORKFLOW_UID
                      value: "{{workflow.uid}}"
                    - name: AWS_ACCESS_KEY_ID
                      valueFrom:
                        secretKeyRef:
                          name: minio-tenant-secret
                          key: accesskey
                    - name: AWS_SECRET_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: minio-tenant-secret
                          key: secretkey
                    - name: AWS_ENDPOINT_URL
                      value: "https://minio-api.storage.internal.opencloudhub.org"
            workerGroupSpecs:
            - replicas: {{inputs.parameters.replicas}}
              groupName: worker
              template:
                spec:
                  ${GPU_NODE_SELECTOR}
                  containers:
                  - name: ray-worker
                    image: {{inputs.parameters.training_image}}
                    resources:
                      requests:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                      limits:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                        ${GPU_RESOURCES}
                    env:
                    - name: MLFLOW_TRACKING_URI
                      value: {{inputs.parameters.mlflow_tracking_uri}}
                    - name: MLFLOW_EXPERIMENT_NAME
                      value: {{inputs.parameters.experiment_name}}
                    - name: MLFLOW_REGISTERED_MODEL_NAME
                      value: "ci.{{inputs.parameters.model_name}}"
                    - name: DVC_DATA_VERSION
                      value: {{inputs.parameters.dvc_data_version}}
                    - name: DOCKER_IMAGE_TAG
                      value: {{inputs.parameters.training_image}}
                    - name: ARGO_WORKFLOW_UID
                      value: "{{workflow.uid}}"
                    - name: AWS_ACCESS_KEY_ID
                      valueFrom:
                        secretKeyRef:
                          name: minio-tenant-secret
                          key: accesskey
                    - name: AWS_SECRET_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: minio-tenant-secret
                          key: secretkey
                    - name: AWS_ENDPOINT_URL
                      value: "https://minio-api.storage.internal.opencloudhub.org"
        EOF

        echo "‚è≥ Waiting for RayJob..."
        sleep 3

        JOB_NAME=$(kubectl get rayjobs -n ai -l workflow-run="${RUN_LABEL}" -o jsonpath='{.items[0].metadata.name}')

        if [ -z "$JOB_NAME" ]; then
          echo "‚ùå Failed to get RayJob"
          exit 1
        fi

        echo "‚úÖ RayJob: $JOB_NAME"
        echo "$JOB_NAME" > /tmp/job-name.txt

        echo "‚è≥ Waiting for job to be submitted..."

        MAX_WAIT=1200
        ELAPSED=0

        while [ $ELAPSED -lt $MAX_WAIT ]; do
          JOB_DEPLOYMENT_STATUS=$(kubectl get rayjob -n ai $JOB_NAME -o jsonpath='{.status.jobDeploymentStatus}')

          if [ "$JOB_DEPLOYMENT_STATUS" = "Running" ] || [ "$JOB_DEPLOYMENT_STATUS" = "Complete" ]; then
            echo "‚úÖ Job submitted successfully"
            break
          fi

          sleep 5
          ELAPSED=$((ELAPSED + 5))
        done

        if [ $ELAPSED -ge $MAX_WAIT ]; then
          echo "‚ùå Job submission timeout after ${MAX_WAIT}s"
          kubectl describe rayjob -n ai $JOB_NAME
          exit 1
        fi

        # Then find the submitter pod
        SUBMITTER_POD=$(kubectl get pods -n ai -l ray.io/is-ray-node!=yes | grep ${JOB_NAME} | awk '{print $1}' | head -n1)

        echo "‚úÖ Submitter pod: $SUBMITTER_POD"
        echo ""
        echo "üîç Streaming training logs..."
        echo "================================"

        # Wait for the submitter pod to be ready
        kubectl wait --for=condition=Ready pod/$SUBMITTER_POD -n ai --timeout=300s || true

        # Stream logs in the background
        kubectl logs -n ai $SUBMITTER_POD -f 2>/dev/null &
        LOG_PID=$!

        # Wait for RayJob to complete
        while true; do
          STATUS=$(kubectl get rayjob -n ai $JOB_NAME -o jsonpath='{.status.jobStatus}' 2>/dev/null)

          # Handle empty status (job not yet started)
          if [ -z "$STATUS" ]; then
            echo "‚è≥ Waiting for job to start..."
            sleep 10
            continue
          fi

          case "$STATUS" in
            "SUCCEEDED")
              sleep 5  # Give logs time to catch up
              kill $LOG_PID 2>/dev/null || true
              wait $LOG_PID 2>/dev/null || true
              echo ""
              echo "‚úÖ Training complete!"
              exit 0
              ;;
            "FAILED"|"STOPPED")
              kill $LOG_PID 2>/dev/null || true
              wait $LOG_PID 2>/dev/null || true
              echo ""
              echo "‚ùå Training failed: $STATUS"
              kubectl describe rayjob -n ai $JOB_NAME
              exit 1
              ;;
            "PENDING"|"RUNNING")
              # Job is still running, continue waiting
              sleep 10
              ;;
            *)
              echo "‚è≥ Job status: $STATUS"
              sleep 10
              ;;
          esac
        done
