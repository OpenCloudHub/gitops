# modules/training.yaml
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: training-templates
  namespace: ai
spec:
  serviceAccountName: workflow-executor

  templates:
  - name: submit-ray-job
    inputs:
      parameters:
      - name: model_name
      - name: experiment_name
      - name: training_image
      - name: training_entrypoint
      - name: training_args
      - name: cpu
      - name: memory
      - name: replicas
      - name: gpu
      - name: mlflow_tracking_uri
      - name: dvc_data_version
    outputs:
      parameters:
      - name: job-name
        valueFrom:
          path: /tmp/job-name.txt
    script:
      image: bitnami/kubectl:latest
      command: [sh]
      source: |
        set -e

        GPU="{{inputs.parameters.gpu}}"

        GPU_RESOURCES=""
        GPU_NODE_SELECTOR=""
        if [ "$GPU" != "0" ]; then
          GPU_RESOURCES="nvidia.com/gpu: \"$GPU\""
          GPU_NODE_SELECTOR="nodeSelector:
          node.opencloudhub.org/gpu: \"true\""
        fi

        RUN_LABEL="wf-$(date +%s)-${RANDOM}"

        echo "Creating RayJob with label: workflow-run=${RUN_LABEL}"

        cat <<EOF | kubectl create -f -
        apiVersion: ray.io/v1
        kind: RayJob
        metadata:
          generateName: {{inputs.parameters.model_name}}-training-
          namespace: ai
          labels:
            workflow-run: "${RUN_LABEL}"
            model: "{{inputs.parameters.model_name}}"
        spec:
          entrypoint: {{inputs.parameters.training_entrypoint}} {{inputs.parameters.training_args}}
          shutdownAfterJobFinishes: true
          ttlSecondsAfterFinished: 300
          rayClusterSpec:
            rayVersion: "2.48.0"
            headGroupSpec:
              serviceType: ClusterIP
              template:
                spec:
                  ${GPU_NODE_SELECTOR}
                  containers:
                  - name: ray-head
                    image: {{inputs.parameters.training_image}}
                    volumeMounts:
                    - name: dvc-config
                      mountPath: /etc/xdg/dvc
                      readOnly: true
                    resources:
                      requests:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                      limits:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                        ${GPU_RESOURCES}
                    env:
                    - name: MLFLOW_TRACKING_URI
                      value: {{inputs.parameters.mlflow_tracking_uri}}
                    - name: MLFLOW_EXPERIMENT_NAME
                      value: {{inputs.parameters.experiment_name}}
                    - name: MLFLOW_REGISTERED_MODEL_NAME
                      value: "ci.{{inputs.parameters.model_name}}"
                    - name: DVC_DATA_VERSION
                      value: {{inputs.parameters.dvc_data_version}}
                    - name: DOCKER_IMAGE_TAG
                      value: {{inputs.parameters.training_image}}
                    - name: ARGO_WORKFLOW_UID
                      value: "{{workflow.uid}}"
                    - name: AWS_ENDPOINT_URL
                      value: "https://minio-api.storage.internal.opencloudhub.org"
                    - name: AWS_CA_BUNDLE
                      value: ""
            workerGroupSpecs:
            - replicas: {{inputs.parameters.replicas}}
              groupName: worker
              template:
                spec:
                  ${GPU_NODE_SELECTOR}
                  containers:
                  - name: ray-worker
                    image: {{inputs.parameters.training_image}}
                    volumeMounts:
                    - name: dvc-config
                      mountPath: /etc/xdg/dvc
                      readOnly: true
                    resources:
                      requests:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                      limits:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                        ${GPU_RESOURCES}
                    env:
                    - name: MLFLOW_TRACKING_URI
                      value: {{inputs.parameters.mlflow_tracking_uri}}
                    - name: MLFLOW_EXPERIMENT_NAME
                      value: {{inputs.parameters.experiment_name}}
                    - name: MLFLOW_REGISTERED_MODEL_NAME
                      value: "ci.{{inputs.parameters.model_name}}"
                    - name: DVC_DATA_VERSION
                      value: {{inputs.parameters.dvc_data_version}}
                    - name: DOCKER_IMAGE_TAG
                      value: {{inputs.parameters.training_image}}
                    - name: ARGO_WORKFLOW_UID
                      value: "{{workflow.uid}}"
                    - name: AWS_ENDPOINT_URL
                      value: "https://minio-api.storage.internal.opencloudhub.org"
                    - name: AWS_CA_BUNDLE
                      value: ""
        EOF

        echo "‚è≥ Waiting for RayJob..."
        sleep 3

        JOB_NAME=$(kubectl get rayjobs -n ai -l workflow-run="${RUN_LABEL}" -o jsonpath='{.items[0].metadata.name}')

        if [ -z "$JOB_NAME" ]; then
          echo "‚ùå Failed to get RayJob"
          exit 1
        fi

        echo "‚úÖ RayJob: $JOB_NAME"
        echo "$JOB_NAME" > /tmp/job-name.txt

        echo "‚è≥ Waiting for job to be submitted..."

        MAX_WAIT=1200
        ELAPSED=0

        while [ $ELAPSED -lt $MAX_WAIT ]; do
          JOB_DEPLOYMENT_STATUS=$(kubectl get rayjob -n ai $JOB_NAME -o jsonpath='{.status.jobDeploymentStatus}')

          if [ "$JOB_DEPLOYMENT_STATUS" = "Running" ] || [ "$JOB_DEPLOYMENT_STATUS" = "Complete" ]; then
            echo "‚úÖ Job submitted successfully"
            break
          fi

          sleep 5
          ELAPSED=$((ELAPSED + 5))
        done

        if [ $ELAPSED -ge $MAX_WAIT ]; then
          echo "‚ùå Job submission timeout after ${MAX_WAIT}s"
          kubectl describe rayjob -n ai $JOB_NAME
          exit 1
        fi

        # Then find the submitter pod
        SUBMITTER_POD=$(kubectl get pods -n ai -l ray.io/is-ray-node!=yes | grep ${JOB_NAME} | awk '{print $1}' | head -n1)

        echo "‚úÖ Submitter pod: $SUBMITTER_POD"
        echo ""
        echo "üîç Streaming training logs..."
        echo "================================"

        kubectl logs -n ai $SUBMITTER_POD -f &
        LOG_PID=$!

        while true; do
          STATUS=$(kubectl get rayjob -n ai $JOB_NAME -o jsonpath='{.status.jobStatus}')

          case "$STATUS" in
            "SUCCEEDED")
              sleep 3
              kill $LOG_PID 2>/dev/null || true
              echo ""
              echo "‚úÖ Training complete!"
              exit 0
              ;;
            "FAILED"|"STOPPED")
              kill $LOG_PID 2>/dev/null || true
              echo ""
              echo "‚ùå Training failed: $STATUS"
              exit 1
              ;;
          esac

          sleep 10
        done
