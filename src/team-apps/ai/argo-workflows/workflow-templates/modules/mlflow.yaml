apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: ai
spec:
  templates:
  - name: compare-and-promote-staging
    inputs:
      parameters:
      - name: model_name
      - name: metric_name
      - name: higher_is_better
      - name: threshold
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: ci_version
        valueFrom:
          path: /tmp/ci_version.txt
      - name: staging_version
        valueFrom:
          path: /tmp/staging_version.txt
      - name: ci_run_id
        valueFrom:
          path: /tmp/ci_run_id.txt
      - name: promoted
        valueFrom:
          path: /tmp/promoted.txt
      - name: reason
        valueFrom:
          path: /tmp/reason.txt
      artifacts:
      - name: comparison-report
        path: /tmp/comparison-report.json
        archive:
          none: {}
      - name: comparison-summary
        path: /tmp/comparison-summary.md
        archive:
          none: {}
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient
        from mlflow.exceptions import MlflowException
        import mlflow
        from datetime import datetime
        import sys
        import json

        def write_failure_outputs(reason, report):
            """Write all required output files for failure cases"""
            report["promoted"] = False
            report["reason"] = reason

            with open("/tmp/promoted.txt", "w") as f:
                f.write("false")
            with open("/tmp/reason.txt", "w") as f:
                f.write(reason)
            with open("/tmp/comparison-report.json", "w") as f:
                json.dump(report, f, indent=2)
            with open("/tmp/comparison-summary.md", "w") as f:
                f.write(f"# ‚ùå Comparison Failed\n\n**Reason:** {reason}\n")

            # Write empty placeholders for required outputs
            with open("/tmp/ci_version.txt", "w") as f:
                f.write("")
            with open("/tmp/staging_version.txt", "w") as f:
                f.write("")
            with open("/tmp/ci_run_id.txt", "w") as f:
                f.write("")

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        mlflow.set_tracking_uri("{{inputs.parameters.mlflow_tracking_uri}}")

        model_name = "{{inputs.parameters.model_name}}"
        metric_name = "{{inputs.parameters.metric_name}}"
        higher_is_better = "{{inputs.parameters.higher_is_better}}" == "true"
        threshold = float("{{inputs.parameters.threshold}}")
        workflow_uid = "{{workflow.uid}}"
        workflow_name = "{{workflow.name}}"

        print(f"üîç Comparing {model_name}")
        print(f"   Workflow: {workflow_uid}")
        print(f"   Metric: {metric_name} ({'higher' if higher_is_better else 'lower'} is better)")
        print(f"   Threshold: {threshold}")

        # Initialize comparison report
        report = {
            "workflow_uid": workflow_uid,
            "workflow_name": workflow_name,
            "model_name": model_name,
            "metric_name": metric_name,
            "higher_is_better": higher_is_better,
            "threshold": threshold,
            "timestamp": datetime.utcnow().isoformat()
        }

        # Get the most recent run from this workflow
        print(f"\nüìä Getting most recent run from workflow {workflow_uid}...")
        runs_df = mlflow.search_runs(
            search_all_experiments=True,
            filter_string=f"tags.argo_workflow_uid = '{workflow_uid}'",
            max_results=1,
            order_by=["start_time DESC"]
        )

        if runs_df.empty:
            reason = f"No runs found with tag argo_workflow_uid='{workflow_uid}'"
            print(f"‚ùå {reason}")
            write_failure_outputs(reason, report)
            sys.exit(1)

        run_id = runs_df.iloc[0]['run_id']
        print(f"   Found run: {run_id[:8]}...")
        report["ci_run_id"] = run_id

        # Get CI model from this run
        print(f"\nüì¶ Looking for CI model from run {run_id[:8]}...")
        ci_versions = client.search_model_versions(
            filter_string=f"name='ci.{model_name}' and run_id='{run_id}'"
        )

        if not ci_versions:
            reason = f"No CI model found from run {run_id}"
            print(f"‚ùå {reason}")
            write_failure_outputs(reason, report)
            sys.exit(1)

        ci_model = ci_versions[0]
        ci_version = ci_model.version
        ci_run_id = ci_model.run_id
        print(f"‚úÖ Found CI model: v{ci_version}")

        report["ci_version"] = str(ci_version)
        report["ci_model_uri"] = f"models:/ci.{model_name}/{ci_version}"

        # Tag the CI model
        print(f"\nüè∑Ô∏è  Tagging CI model...")
        ci_tags = {
            "argo_workflow_uid": workflow_uid,
            "argo_workflow_name": workflow_name,
            "tagged_by": "ci_pipeline",
            "tagged_at": datetime.utcnow().isoformat()
        }

        for key, value in ci_tags.items():
            client.set_model_version_tag(f"ci.{model_name}", ci_version, key, value)

        # Get CI metrics
        print(f"\nüìä Getting metrics from CI model...")
        ci_run = client.get_run(ci_run_id)
        if metric_name not in ci_run.data.metrics:
            reason = f"Metric '{metric_name}' not found in CI model run"
            print(f"‚ùå {reason}")
            client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "rejected")
            client.set_model_version_tag(f"ci.{model_name}", ci_version, "rejection_reason", reason)
            write_failure_outputs(reason, report)
            sys.exit(1)

        ci_value = ci_run.data.metrics[metric_name]
        print(f"   CI {metric_name}: {ci_value}")
        report["ci_metric_value"] = ci_value

        # Try to get champion
        print(f"\nüèÜ Looking for champion...")
        should_promote = False
        champion_version = None
        champion_value = None
        improvement = None

        try:
            champion = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
            champion_version = champion.version
            print(f"   Found champion: v{champion_version}")
            report["champion_version"] = str(champion_version)
            report["champion_model_uri"] = f"models:/prod.{model_name}/{champion_version}"

            champ_run = client.get_run(champion.run_id)
            if metric_name not in champ_run.data.metrics:
                reason = f"Champion missing metric '{metric_name}', promoting CI model as replacement"
                print(f"   {reason}")
                should_promote = True
                improvement = "N/A"
                report["improvement"] = improvement
                report["reason"] = reason
            else:
                champion_value = champ_run.data.metrics[metric_name]
                print(f"   Champion {metric_name}: {champion_value}")
                report["champion_metric_value"] = champion_value

                if higher_is_better:
                    improvement = ci_value - champion_value
                    improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0
                else:
                    improvement = champion_value - ci_value
                    improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0

                print(f"   Improvement: {improvement:.6f} ({improvement_pct:.2f}%)")
                report["improvement"] = improvement
                report["improvement_pct"] = improvement_pct

                if improvement >= threshold:
                    reason = f"CI model outperforms champion by {improvement:.6f} ({improvement_pct:.2f}%), exceeds threshold {threshold}"
                    print(f"‚úÖ {reason}")
                    should_promote = True
                    report["reason"] = reason
                else:
                    reason = f"Insufficient improvement: {improvement:.6f} ({improvement_pct:.2f}%) < threshold {threshold}"
                    print(f"‚ùå {reason}")
                    client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "rejected")
                    client.set_model_version_tag(f"ci.{model_name}", ci_version, "rejection_reason", reason)

                    report["promoted"] = False
                    report["reason"] = reason

                    # Write all outputs including required empty ones
                    with open("/tmp/promoted.txt", "w") as f:
                        f.write("false")
                    with open("/tmp/reason.txt", "w") as f:
                        f.write(reason)
                    with open("/tmp/comparison-report.json", "w") as f:
                        json.dump(report, f, indent=2)
                    with open("/tmp/ci_version.txt", "w") as f:
                        f.write(str(ci_version))
                    with open("/tmp/staging_version.txt", "w") as f:
                        f.write("")
                    with open("/tmp/ci_run_id.txt", "w") as f:
                        f.write(ci_run_id)

                    # Create markdown summary
                    with open("/tmp/comparison-summary.md", "w") as f:
                        f.write(f"# ‚ùå Model Not Promoted\n\n")
                        f.write(f"## Comparison Results\n\n")
                        f.write(f"- **Model:** `{model_name}`\n")
                        f.write(f"- **Metric:** `{metric_name}` ({'higher' if higher_is_better else 'lower'} is better)\n")
                        f.write(f"- **CI Model:** `v{ci_version}` = **{ci_value:.6f}**\n")
                        f.write(f"- **Champion:** `v{champion_version}` = **{champion_value:.6f}**\n")
                        f.write(f"- **Improvement:** {improvement:.6f} ({improvement_pct:.2f}%)\n")
                        f.write(f"- **Required Threshold:** {threshold}\n\n")
                        f.write(f"## Decision\n\n")
                        f.write(f"‚õî **Not promoted:** {reason}\n")

                    print(f"‚úÖ Comparison complete - model not promoted (below threshold)")

        except MlflowException as e:
            # No champion exists - this is expected for first deployment
            reason = "No champion exists, promoting first model"
            print(f"   {reason}")
            should_promote = True
            improvement = "N/A (first deployment)"
            report["champion_version"] = None
            report["champion_metric_value"] = None
            report["improvement"] = improvement
            report["reason"] = reason

        if not should_promote:
            sys.exit(1)

        # Copy to staging
        print(f"\nüì¶ Copying ci.{model_name}/v{ci_version} ‚Üí staging.{model_name}")
        staging_mv = client.copy_model_version(
            f"models:/ci.{model_name}/{ci_version}",
            f"staging.{model_name}"
        )

        print(f"‚úÖ Created staging.{model_name}/v{staging_mv.version}")
        report["staging_version"] = str(staging_mv.version)
        report["staging_model_uri"] = f"models:/staging.{model_name}/{staging_mv.version}"
        report["promoted"] = True

        # Write all outputs
        with open("/tmp/ci_version.txt", "w") as f:
            f.write(str(ci_version))
        with open("/tmp/staging_version.txt", "w") as f:
            f.write(str(staging_mv.version))
        with open("/tmp/ci_run_id.txt", "w") as f:
            f.write(ci_run_id)
        with open("/tmp/promoted.txt", "w") as f:
            f.write("true")
        with open("/tmp/reason.txt", "w") as f:
            f.write(report["reason"])
        with open("/tmp/comparison-report.json", "w") as f:
            json.dump(report, f, indent=2)

        # Create markdown summary for success
        with open("/tmp/comparison-summary.md", "w") as f:
            f.write(f"# ‚úÖ Model Promoted to Staging\n\n")
            f.write(f"## Comparison Results\n\n")
            f.write(f"- **Model:** `{model_name}`\n")
            f.write(f"- **Metric:** `{metric_name}` ({'higher' if higher_is_better else 'lower'} is better)\n")
            f.write(f"- **CI Model:** `v{ci_version}` = **{ci_value:.6f}**\n")

            if champion_version:
                f.write(f"- **Champion:** `v{champion_version}` = **{champion_value:.6f}**\n")
                if isinstance(improvement, float):
                    f.write(f"- **Improvement:** {improvement:.6f} ({report.get('improvement_pct', 0):.2f}%)\n")
                else:
                    f.write(f"- **Improvement:** {improvement}\n")
            else:
                f.write(f"- **Champion:** None (first deployment)\n")

            f.write(f"- **Required Threshold:** {threshold}\n\n")
            f.write(f"## Decision\n\n")
            f.write(f"‚úÖ **Promoted:** {report['reason']}\n\n")
            f.write(f"## Next Steps\n\n")
            f.write(f"- Model copied to staging: `staging.{model_name}/v{staging_mv.version}`\n")
            f.write(f"- Awaiting approval for production deployment\n")

        print(f"\n‚úÖ Comparison complete - model promoted to staging")

  - name: test-staging-model
    inputs:
      parameters:
      - name: model_name
      - name: staging_version
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: test_passed
        valueFrom:
          path: /tmp/test_passed.txt
      artifacts:
      - name: test-report
        path: /tmp/test-report.md
        archive:
          none: {}
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient
        from datetime import datetime
        import random

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        model_name = "{{inputs.parameters.model_name}}"
        staging_version = "{{inputs.parameters.staging_version}}"

        print(f"üß™ Testing staging model: {model_name}/v{staging_version}")

        # Simulate test execution
        test_results = {
            "model_loading": True,
            "prediction_latency": random.uniform(10, 50),  # ms
            "prediction_accuracy_sample": random.uniform(0.85, 0.95),
            "schema_validation": True,
            "smoke_test": True
        }

        all_passed = all([
            test_results["model_loading"],
            test_results["prediction_latency"] < 100,
            test_results["prediction_accuracy_sample"] > 0.8,
            test_results["schema_validation"],
            test_results["smoke_test"]
        ])

        # Create test report
        with open("/tmp/test-report.md", "w") as f:
            f.write(f"# üß™ Staging Model Test Report\n\n")
            f.write(f"**Model:** `staging.{model_name}/v{staging_version}`  \n")
            f.write(f"**Tested:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC  \n")
            f.write(f"**Status:** {'‚úÖ PASSED' if all_passed else '‚ùå FAILED'}\n\n")

            f.write(f"## Test Suite\n\n")
            f.write(f"### 1. Model Loading\n")
            f.write(f"- **Result:** {'‚úÖ Pass' if test_results['model_loading'] else '‚ùå Fail'}\n")
            f.write(f"- **Details:** Model successfully loaded from MLflow registry\n\n")

            f.write(f"### 2. Prediction Latency\n")
            f.write(f"- **Result:** {'‚úÖ Pass' if test_results['prediction_latency'] < 100 else '‚ùå Fail'}\n")
            f.write(f"- **Latency:** {test_results['prediction_latency']:.2f}ms (threshold: <100ms)\n\n")

            f.write(f"### 3. Sample Prediction Accuracy\n")
            f.write(f"- **Result:** {'‚úÖ Pass' if test_results['prediction_accuracy_sample'] > 0.8 else '‚ùå Fail'}\n")
            f.write(f"- **Accuracy:** {test_results['prediction_accuracy_sample']:.2%} (threshold: >80%)\n\n")

            f.write(f"### 4. Schema Validation\n")
            f.write(f"- **Result:** {'‚úÖ Pass' if test_results['schema_validation'] else '‚ùå Fail'}\n")
            f.write(f"- **Details:** Input/output schemas validated\n\n")

            f.write(f"### 5. Smoke Test\n")
            f.write(f"- **Result:** {'‚úÖ Pass' if test_results['smoke_test'] else '‚ùå Fail'}\n")
            f.write(f"- **Details:** Basic inference working correctly\n\n")

            f.write(f"---\n\n")
            f.write(f"## Integration Points\n\n")
            f.write(f"This is a **placeholder test suite**. Replace with:\n\n")
            f.write(f"- **Load testing:** Use Locust or K6 for realistic traffic simulation\n")
            f.write(f"- **A/B testing:** Compare against current production model\n")
            f.write(f"- **Data drift detection:** Validate model performance on recent data\n")
            f.write(f"- **Integration tests:** Test full prediction pipeline end-to-end\n")
            f.write(f"- **Shadow deployment:** Route production traffic to staging for comparison\n\n")

        with open("/tmp/test_passed.txt", "w") as f:
            f.write("true" if all_passed else "false")

        if all_passed:
            print(f"‚úÖ All tests passed!")
        else:
            print(f"‚ùå Some tests failed")
            exit(1)

  - name: promote-to-production
    inputs:
      parameters:
      - name: model_name
      - name: staging_version
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: prod_version
        valueFrom:
          path: /tmp/prod_version.txt
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        model_name = "{{inputs.parameters.model_name}}"
        staging_version = "{{inputs.parameters.staging_version}}"

        print(f"üöÄ Promoting to production: {model_name}")

        prod_mv = client.copy_model_version(
            f"models:/staging.{model_name}/{staging_version}",
            f"prod.{model_name}"
        )

        print(f"‚úÖ Created prod.{model_name}/v{prod_mv.version}")

        try:
            old_champ = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
            print(f"   Moving old champion v{old_champ.version} ‚Üí @previous")
            client.set_registered_model_alias(f"prod.{model_name}", "previous", old_champ.version)
        except:
            print("   No previous champion")

        client.set_registered_model_alias(f"prod.{model_name}", "champion", prod_mv.version)
        print(f"‚úÖ Set prod.{model_name}@champion ‚Üí v{prod_mv.version}")

        with open("/tmp/prod_version.txt", "w") as f:
            f.write(prod_mv.version)

  - name: tag-mlflow-prod-model
    inputs:
      parameters:
      - name: model_name
      - name: prod_version
      - name: staging_version
      - name: ci_version
      - name: approval_mode
      - name: mlflow_tracking_uri
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient
        from datetime import datetime

        tracking_uri = "{{inputs.parameters.mlflow_tracking_uri}}"
        client = MlflowClient(tracking_uri)
        model_name = "{{inputs.parameters.model_name}}"
        prod_version = "{{inputs.parameters.prod_version}}"

        tags = {
            "argo_workflow_name": "{{workflow.name}}",
            "argo_workflow_uid": "{{workflow.uid}}",
            "deployment_timestamp": datetime.utcnow().isoformat(),
            "approval_mode": "{{inputs.parameters.approval_mode}}",
        }

        for key, value in tags.items():
            client.set_model_version_tag(f"prod.{model_name}", prod_version, key, value)

        print("‚úÖ Deployment info tagged!")
