apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlflow-templates
  namespace: ai
spec:
  templates:
  - name: compare-and-promote
    inputs:
      parameters:
      - name: model_name
      - name: metric_name
      - name: higher_is_better
      - name: threshold
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: ci_version
        valueFrom:
          path: /tmp/ci_version.txt
      - name: staging_version
        valueFrom:
          path: /tmp/staging_version.txt
      - name: ci_run_id
        valueFrom:
          path: /tmp/ci_run_id.txt
      - name: promoted
        valueFrom:
          path: /tmp/promoted.txt
      - name: reason
        valueFrom:
          path: /tmp/reason.txt
      artifacts:
      - name: comparison-report
        path: /tmp/comparison-report.json
        archive:
          none: {}
      - name: comparison-summary
        path: /tmp/comparison-summary.md
        archive:
          none: {}
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient
        import mlflow
        from datetime import datetime
        import sys
        import json

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        mlflow.set_tracking_uri("{{inputs.parameters.mlflow_tracking_uri}}")

        model_name = "{{inputs.parameters.model_name}}"
        metric_name = "{{inputs.parameters.metric_name}}"
        higher_is_better = "{{inputs.parameters.higher_is_better}}" == "true"
        threshold = float("{{inputs.parameters.threshold}}")
        workflow_uid = "{{workflow.uid}}"
        workflow_name = "{{workflow.name}}"

        print(f"ðŸ” Comparing {model_name}")
        print(f"   Workflow: {workflow_uid}")
        print(f"   Metric: {metric_name} ({'higher' if higher_is_better else 'lower'} is better)")
        print(f"   Threshold: {threshold}")

        # Initialize comparison report
        report = {
            "workflow_uid": workflow_uid,
            "workflow_name": workflow_name,
            "model_name": model_name,
            "metric_name": metric_name,
            "higher_is_better": higher_is_better,
            "threshold": threshold,
            "timestamp": datetime.utcnow().isoformat(),
            "promoted": False,
            "reason": "Unknown"
        }

        # Get the most recent run from this workflow
        print(f"\nðŸ“Š Getting most recent run from workflow {workflow_uid}...")
        runs_df = mlflow.search_runs(
            search_all_experiments=True,
            filter_string=f"tags.argo_workflow_uid = '{workflow_uid}'",
            max_results=1,
            order_by=["start_time DESC"]
        )

        if runs_df.empty:
            reason = f"No runs found with tag argo_workflow_uid='{workflow_uid}'"
            print(f"âŒ {reason}")
            report["reason"] = reason
            with open("/tmp/comparison-report.json", "w") as f:
                json.dump(report, f, indent=2)
            with open("/tmp/reason.txt", "w") as f:
                f.write(reason)
            with open("/tmp/promoted.txt", "w") as f:
                f.write("false")
            with open("/tmp/comparison-summary.md", "w") as f:
                f.write(f"# âŒ Comparison Failed\n\n**Reason:** {reason}\n")
            sys.exit(1)

        run_id = runs_df.iloc[0]['run_id']
        print(f"   Found run: {run_id[:8]}...")
        report["ci_run_id"] = run_id

        # Get CI model from this run
        print(f"\nðŸ“¦ Looking for CI model from run {run_id[:8]}...")
        ci_versions = client.search_model_versions(
            filter_string=f"name='ci.{model_name}' and run_id='{run_id}'"
        )

        if not ci_versions:
            reason = f"No CI model found from run {run_id}"
            print(f"âŒ {reason}")
            report["reason"] = reason
            with open("/tmp/comparison-report.json", "w") as f:
                json.dump(report, f, indent=2)
            with open("/tmp/reason.txt", "w") as f:
                f.write(reason)
            with open("/tmp/promoted.txt", "w") as f:
                f.write("false")
            with open("/tmp/comparison-summary.md", "w") as f:
                f.write(f"# âŒ Comparison Failed\n\n**Reason:** {reason}\n")
            sys.exit(1)

        ci_model = ci_versions[0]
        ci_version = ci_model.version
        ci_run_id = ci_model.run_id
        print(f"âœ… Found CI model: v{ci_version}")

        report["ci_version"] = str(ci_version)
        report["ci_model_uri"] = f"models:/ci.{model_name}/{ci_version}"

        # Tag the CI model
        print(f"\nðŸ·ï¸  Tagging CI model...")
        ci_tags = {
            "argo_workflow_uid": workflow_uid,
            "argo_workflow_name": workflow_name,
            "tagged_by": "ci_pipeline",
            "tagged_at": datetime.utcnow().isoformat()
        }

        for key, value in ci_tags.items():
            client.set_model_version_tag(f"ci.{model_name}", ci_version, key, value)

        # Get CI metrics
        print(f"\nðŸ“Š Getting metrics from CI model...")
        ci_run = client.get_run(ci_run_id)
        if metric_name not in ci_run.data.metrics:
            reason = f"Metric '{metric_name}' not found in CI model run"
            print(f"âŒ {reason}")
            client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "rejected")
            client.set_model_version_tag(f"ci.{model_name}", ci_version, "rejection_reason", reason)
            report["reason"] = reason
            with open("/tmp/comparison-report.json", "w") as f:
                json.dump(report, f, indent=2)
            with open("/tmp/reason.txt", "w") as f:
                f.write(reason)
            with open("/tmp/promoted.txt", "w") as f:
                f.write("false")
            with open("/tmp/comparison-summary.md", "w") as f:
                f.write(f"# âŒ Comparison Failed\n\n**Reason:** {reason}\n")
            sys.exit(1)

        ci_value = ci_run.data.metrics[metric_name]
        print(f"   CI {metric_name}: {ci_value}")
        report["ci_metric_value"] = ci_value

        # Try to get champion
        print(f"\nðŸ† Looking for champion...")
        should_promote = False
        champion_info = None
        improvement = None
        champion_version = None
        champion_value = None

        try:
            champion = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
            champion_version = champion.version
            print(f"   Found champion: v{champion_version}")
            champion_info = f"prod.{model_name} v{champion_version}"
            report["champion_version"] = str(champion_version)
            report["champion_model_uri"] = f"models:/prod.{model_name}/{champion_version}"

            champ_run = client.get_run(champion.run_id)
            if metric_name not in champ_run.data.metrics:
                reason = f"Champion missing metric '{metric_name}', promoting CI model as replacement"
                print(f"   {reason}")
                should_promote = True
                improvement = "N/A"
                report["improvement"] = improvement
                report["reason"] = reason
            else:
                champion_value = champ_run.data.metrics[metric_name]
                print(f"   Champion {metric_name}: {champion_value}")
                report["champion_metric_value"] = champion_value

                if higher_is_better:
                    improvement = ci_value - champion_value
                    improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0
                else:
                    improvement = champion_value - ci_value
                    improvement_pct = (improvement / champion_value * 100) if champion_value != 0 else 0

                print(f"   Improvement: {improvement:.6f} ({improvement_pct:.2f}%)")
                report["improvement"] = improvement
                report["improvement_pct"] = improvement_pct

                if improvement >= threshold:
                    reason = f"CI model outperforms champion by {improvement:.6f} ({improvement_pct:.2f}%), exceeds threshold {threshold}"
                    print(f"âœ… {reason}")
                    should_promote = True
                    report["reason"] = reason
                else:
                    reason = f"Insufficient improvement: {improvement:.6f} ({improvement_pct:.2f}%) < threshold {threshold}"
                    print(f"âŒ {reason}")
                    client.set_model_version_tag(f"ci.{model_name}", ci_version, "promotion_status", "rejected")
                    client.set_model_version_tag(f"ci.{model_name}", ci_version, "rejection_reason", reason)
                    report["reason"] = reason
                    report["promoted"] = False

                    # Write outputs for failed comparison
                    with open("/tmp/comparison-report.json", "w") as f:
                        json.dump(report, f, indent=2)
                    with open("/tmp/reason.txt", "w") as f:
                        f.write(reason)
                    with open("/tmp/promoted.txt", "w") as f:
                        f.write("false")

                    # Create markdown summary
                    with open("/tmp/comparison-summary.md", "w") as f:
                        f.write(f"# âŒ Model Not Promoted\n\n")
                        f.write(f"## Comparison Results\n\n")
                        f.write(f"- **Model:** `{model_name}`\n")
                        f.write(f"- **Metric:** `{metric_name}` ({'higher' if higher_is_better else 'lower'} is better)\n")
                        f.write(f"- **CI Model:** `v{ci_version}` = **{ci_value:.6f}**\n")
                        f.write(f"- **Champion:** `v{champion_version}` = **{champion_value:.6f}**\n")
                        f.write(f"- **Improvement:** {improvement:.6f} ({improvement_pct:.2f}%)\n")
                        f.write(f"- **Required Threshold:** {threshold}\n\n")
                        f.write(f"## Decision\n\n")
                        f.write(f"â›” **Not promoted:** {reason}\n")

                    sys.exit(1)
        except Exception as e:
            if "RESOURCE_DOES_NOT_EXIST" in str(e):
                reason = "No champion exists, promoting first model"
                print(f"   {reason}")
                should_promote = True
                champion_info = "none (first model)"
                improvement = "N/A"
                report["champion_version"] = None
                report["champion_metric_value"] = None
                report["improvement"] = improvement
                report["reason"] = reason
            else:
                reason = f"Error checking champion: {str(e)}"
                print(f"âŒ {reason}")
                report["reason"] = reason
                with open("/tmp/comparison-report.json", "w") as f:
                    json.dump(report, f, indent=2)
                with open("/tmp/reason.txt", "w") as f:
                    f.write(reason)
                with open("/tmp/promoted.txt", "w") as f:
                    f.write("false")
                with open("/tmp/comparison-summary.md", "w") as f:
                    f.write(f"# âŒ Comparison Failed\n\n**Reason:** {reason}\n")
                sys.exit(1)

        if not should_promote:
            sys.exit(1)

        # Copy to staging
        print(f"\nðŸ“¦ Copying ci.{model_name}/v{ci_version} â†’ staging.{model_name}")
        staging_mv = client.copy_model_version(
            f"models:/ci.{model_name}/{ci_version}",
            f"staging.{model_name}"
        )

        print(f"âœ… Created staging.{model_name}/v{staging_mv.version}")
        report["staging_version"] = str(staging_mv.version)
        report["staging_model_uri"] = f"models:/staging.{model_name}/{staging_mv.version}"
        report["promoted"] = True

        # Write all outputs
        with open("/tmp/ci_version.txt", "w") as f:
            f.write(str(ci_version))
        with open("/tmp/staging_version.txt", "w") as f:
            f.write(str(staging_mv.version))
        with open("/tmp/ci_run_id.txt", "w") as f:
            f.write(ci_run_id)
        with open("/tmp/promoted.txt", "w") as f:
            f.write("true")
        with open("/tmp/reason.txt", "w") as f:
            f.write(report["reason"])
        with open("/tmp/comparison-report.json", "w") as f:
            json.dump(report, f, indent=2)

        # Create markdown summary for success
        with open("/tmp/comparison-summary.md", "w") as f:
            f.write(f"# âœ… Model Promoted to Staging\n\n")
            f.write(f"## Comparison Results\n\n")
            f.write(f"- **Model:** `{model_name}`\n")
            f.write(f"- **Metric:** `{metric_name}` ({'higher' if higher_is_better else 'lower'} is better)\n")
            f.write(f"- **CI Model:** `v{ci_version}` = **{ci_value:.6f}**\n")

            if champion_version:
                f.write(f"- **Champion:** `v{champion_version}` = **{champion_value:.6f}**\n")
                if isinstance(improvement, float):
                    f.write(f"- **Improvement:** {improvement:.6f} ({report.get('improvement_pct', 0):.2f}%)\n")
                else:
                    f.write(f"- **Improvement:** {improvement}\n")
            else:
                f.write(f"- **Champion:** None (first deployment)\n")

            f.write(f"- **Required Threshold:** {threshold}\n\n")
            f.write(f"## Decision\n\n")
            f.write(f"âœ… **Promoted:** {report['reason']}\n\n")
            f.write(f"## Next Steps\n\n")
            f.write(f"- Model copied to staging: `staging.{model_name}/v{staging_mv.version}`\n")
            f.write(f"- Awaiting approval for production deployment\n")

        print(f"\nâœ… Comparison complete - model promoted to staging")

  - name: promote-to-production
    inputs:
      parameters:
      - name: model_name
      - name: staging_version
      - name: mlflow_tracking_uri
    outputs:
      parameters:
      - name: prod_version
        valueFrom:
          path: /tmp/prod_version.txt
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient

        client = MlflowClient("{{inputs.parameters.mlflow_tracking_uri}}")
        model_name = "{{inputs.parameters.model_name}}"
        staging_version = "{{inputs.parameters.staging_version}}"

        print(f"ðŸš€ Promoting to production: {model_name}")

        prod_mv = client.copy_model_version(
            f"models:/staging.{model_name}/{staging_version}",
            f"prod.{model_name}"
        )

        print(f"âœ… Created prod.{model_name}/v{prod_mv.version}")

        try:
            old_champ = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
            print(f"   Moving old champion v{old_champ.version} â†’ @previous")
            client.set_registered_model_alias(f"prod.{model_name}", "previous", old_champ.version)
        except:
            print("   No previous champion")

        client.set_registered_model_alias(f"prod.{model_name}", "champion", prod_mv.version)
        print(f"âœ… Set prod.{model_name}@champion â†’ v{prod_mv.version}")

        with open("/tmp/prod_version.txt", "w") as f:
            f.write(prod_mv.version)

  - name: tag-deployment-ready
    inputs:
      parameters:
      - name: model_name
      - name: prod_version
      - name: staging_version
      - name: ci_version
      - name: approval_mode
      - name: mlflow_tracking_uri
    script:
      image: ghcr.io/mlflow/mlflow:v2.18.0
      command: [python]
      source: |
        from mlflow import MlflowClient
        from datetime import datetime

        tracking_uri = "{{inputs.parameters.mlflow_tracking_uri}}"
        client = MlflowClient(tracking_uri)
        model_name = "{{inputs.parameters.model_name}}"
        prod_version = "{{inputs.parameters.prod_version}}"

        # TODO: do we still need these or tag in code?
        tags = {
            "argo_workflow_name": "{{workflow.name}}",
            "argo_workflow_uid": "{{workflow.uid}}",
            "deployment_timestamp": datetime.utcnow().isoformat(),
            "approval_mode": "{{inputs.parameters.approval_mode}}",
        }

        for key, value in tags.items():
            client.set_model_version_tag(f"prod.{model_name}", prod_version, key, value)

        print("âœ… Deployment info tagged!")
