apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: mlops-pipeline
  namespace: ai
spec:
  serviceAccountName: workflow-executor

  arguments:
    parameters:
    - name: model_name
      description: "Model name (e.g., wine-classifier)"
    - name: experiment_name
      description: "MLflow experiment name"
    - name: compute_type
      enum: [cpu-small, cpu-medium, cpu-large, gpu-small, gpu-medium, gpu-large]
      description: "Compute configuration"
    - name: approval_mode
      value: "manual"
      enum: [manual, automatic]
    - name: comparison_metric
      value: "accuracy"
      description: "Metric to compare"
    - name: higher_is_better
      value: "true"
      description: "Whether higher metric values are better (true/false)"
    - name: comparison_threshold
      value: "0.05"
      description: "Minimum improvement threshold"
    - name: training_image
      description: "Container image with training code"
    - name: training_entrypoint
      value: "python src/training/train.py"
    - name: training_args
      value: ""
    - name: mlflow_tracking_uri
      value: "http://mlflow.ai.svc.cluster.local:80"
    - name: docker_registry
      value: "docker.io/yourusername"

  entrypoint: main

  templates:
  - name: main
    steps:
    # Phase 1: Map compute config
    - - name: map-compute
        template: map-compute-config
        arguments:
          parameters:
          - name: compute_type
            value: "{{workflow.parameters.compute_type}}"

    # Phase 2: Submit Ray job
    - - name: submit-training
        template: submit-ray-job
        arguments:
          parameters:
          - name: model_name
            value: "{{workflow.parameters.model_name}}"
          - name: experiment_name
            value: "{{workflow.parameters.experiment_name}}"
          - name: training_image
            value: "{{workflow.parameters.training_image}}"
          - name: training_entrypoint
            value: "{{workflow.parameters.training_entrypoint}}"
          - name: training_args
            value: "{{workflow.parameters.training_args}}"
          - name: cpu
            value: "{{steps.map-compute.outputs.parameters.cpu}}"
          - name: memory
            value: "{{steps.map-compute.outputs.parameters.memory}}"
          - name: replicas
            value: "{{steps.map-compute.outputs.parameters.replicas}}"
          - name: gpu
            value: "{{steps.map-compute.outputs.parameters.gpu}}"

    # # Phase 3: Wait for completion
    # - - name: wait-training
    #     template: wait-ray-job
    #     arguments:
    #       parameters:
    #       - name: job_name
    #         value: "{{steps.submit-training.outputs.parameters.job-name}}"

    # # Phase 4: Compare and promote to staging
    # - - name: compare-promote
    #     template: compare-and-promote
    #     arguments:
    #       parameters:
    #       - name: model_name
    #         value: "{{workflow.parameters.model_name}}"
    #       - name: metric
    #         value: "{{workflow.parameters.comparison_metric}}"
    #       - name: higher_is_better
    #         value: "{{workflow.parameters.higher_is_better}}"
    #       - name: threshold
    #         value: "{{workflow.parameters.comparison_threshold}}"
    #       - name: mlflow_uri
    #         value: "{{workflow.parameters.mlflow_tracking_uri}}"

    # # Phase 5: Build staging image
    # - - name: build-staging
    #     template: build-staging-image
    #     arguments:
    #       parameters:
    #       - name: model_name
    #         value: "{{workflow.parameters.model_name}}"
    #       - name: staging_version
    #         value: "{{steps.compare-promote.outputs.parameters.staging_version}}"
    #       - name: registry
    #         value: "{{workflow.parameters.docker_registry}}"
    #       - name: mlflow_uri
    #         value: "{{workflow.parameters.mlflow_tracking_uri}}"

    # # Phase 6: Test staging
    # - - name: test-staging
    #     template: test-staging
    #     arguments:
    #       parameters:
    #       - name: image_tag
    #         value: "{{steps.build-staging.outputs.parameters.staging_image_tag}}"

    # # Phase 7: Approval
    # - - name: approval
    #     suspend:
    #       duration: "48h"
    #     when: "{{workflow.parameters.approval_mode}} == manual"

    # # Phase 8: Promote to production
    # - - name: promote-production
    #     template: promote-to-production
    #     arguments:
    #       parameters:
    #       - name: model_name
    #         value: "{{workflow.parameters.model_name}}"
    #       - name: staging_version
    #         value: "{{steps.compare-promote.outputs.parameters.staging_version}}"
    #       - name: mlflow_uri
    #         value: "{{workflow.parameters.mlflow_tracking_uri}}"

    # # Phase 9: Retag image
    # - - name: retag-production
    #     template: retag-image
    #     arguments:
    #       parameters:
    #       - name: staging_image
    #         value: "{{steps.build-staging.outputs.parameters.staging_image_tag}}"

  # ========================================================================
  # TEMPLATE DEFINITIONS
  # ========================================================================

  # Map compute type to resources
  - name: map-compute-config
    inputs:
      parameters:
      - name: compute_type
    outputs:
      parameters:
      - name: cpu
        valueFrom:
          path: /tmp/cpu.txt
      - name: memory
        valueFrom:
          path: /tmp/memory.txt
      - name: replicas
        valueFrom:
          path: /tmp/replicas.txt
      - name: gpu
        valueFrom:
          path: /tmp/gpu.txt
    script:
      image: alpine:latest
      command: [sh]
      source: |
        #!/bin/sh
        set -e

        case "{{inputs.parameters.compute_type}}" in
          cpu-small)
            echo "2" > /tmp/cpu.txt
            echo "4Gi" > /tmp/memory.txt
            echo "1" > /tmp/replicas.txt
            echo "0" > /tmp/gpu.txt
            ;;
          cpu-medium)
            echo "4" > /tmp/cpu.txt
            echo "8Gi" > /tmp/memory.txt
            echo "1" > /tmp/replicas.txt
            echo "0" > /tmp/gpu.txt
            ;;
          cpu-large)
            echo "8" > /tmp/cpu.txt
            echo "16Gi" > /tmp/memory.txt
            echo "2" > /tmp/replicas.txt
            echo "0" > /tmp/gpu.txt
            ;;
          gpu-small)
            echo "4" > /tmp/cpu.txt
            echo "16Gi" > /tmp/memory.txt
            echo "1" > /tmp/replicas.txt
            echo "1" > /tmp/gpu.txt
            ;;
          gpu-medium)
            echo "8" > /tmp/cpu.txt
            echo "32Gi" > /tmp/memory.txt
            echo "1" > /tmp/replicas.txt
            echo "2" > /tmp/gpu.txt
            ;;
          gpu-large)
            echo "16" > /tmp/cpu.txt
            echo "64Gi" > /tmp/memory.txt
            echo "1" > /tmp/replicas.txt
            echo "4" > /tmp/gpu.txt
            ;;
          *)
            echo "Unknown compute type: {{inputs.parameters.compute_type}}"
            exit 1
            ;;
        esac

        echo "‚úÖ Mapped {{inputs.parameters.compute_type}}"

  # Submit RayJob
  - name: submit-ray-job
    inputs:
      parameters:
      - name: model_name
      - name: experiment_name
      - name: training_image
      - name: training_entrypoint
      - name: training_args
      - name: cpu
      - name: memory
      - name: replicas
      - name: gpu
    outputs:
      parameters:
      - name: job-name
        valueFrom:
          path: /tmp/job-name.txt
    script:
      image: bitnami/kubectl:latest
      command: [sh]
      source: |
        set -e

        GPU="{{inputs.parameters.gpu}}"

        # Build GPU resources block if needed
        GPU_RESOURCES=""
        GPU_NODE_SELECTOR=""
        if [ "$GPU" != "0" ]; then
          GPU_RESOURCES="nvidia.com/gpu: \"$GPU\""
          GPU_NODE_SELECTOR="nodeSelector:
          node.opencloudhub.org/gpu: \"true\""
        fi

        # Create unique label for this workflow run (no $$ - use seconds + random)
        RUN_LABEL="wf-$(date +%s)-${RANDOM}"

        echo "Creating RayJob with label: workflow-run=${RUN_LABEL}"

        cat <<EOF | kubectl create -f -
        apiVersion: ray.io/v1
        kind: RayJob
        metadata:
          generateName: {{inputs.parameters.model_name}}-training-
          namespace: ai
          labels:
            workflow-run: "${RUN_LABEL}"
            model: "{{inputs.parameters.model_name}}"
        spec:
          entrypoint: {{inputs.parameters.training_entrypoint}} {{inputs.parameters.training_args}}
          shutdownAfterJobFinishes: true
          ttlSecondsAfterFinished: 300
          rayClusterSpec:
            rayVersion: "2.48.0"
            headGroupSpec:
              serviceType: ClusterIP
              template:
                spec:
                  ${GPU_NODE_SELECTOR}
                  containers:
                  - name: ray-head
                    image: {{inputs.parameters.training_image}}
                    resources:
                      requests:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                      limits:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                        ${GPU_RESOURCES}
                    env:
                    - name: MLFLOW_TRACKING_URI
                      value: http://mlflow.ai.svc.cluster.local:5000
                    - name: MLFLOW_EXPERIMENT_NAME
                      value: {{inputs.parameters.experiment_name}}
                    - name: MLFLOW_REGISTERED_MODEL_NAME
                      value: {{inputs.parameters.model_name}}
            workerGroupSpecs:
            - replicas: {{inputs.parameters.replicas}}
              groupName: worker
              template:
                spec:
                  ${GPU_NODE_SELECTOR}
                  containers:
                  - name: ray-worker
                    image: {{inputs.parameters.training_image}}
                    resources:
                      requests:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                      limits:
                        cpu: "{{inputs.parameters.cpu}}"
                        memory: "{{inputs.parameters.memory}}"
                        ${GPU_RESOURCES}
                    env:
                    - name: MLFLOW_TRACKING_URI
                      value: http://mlflow.ai.svc.cluster.local:5000
                    - name: MLFLOW_EXPERIMENT_NAME
                      value: {{inputs.parameters.experiment_name}}
                    - name: MLFLOW_REGISTERED_MODEL_NAME
                      value: {{inputs.parameters.model_name}}
        EOF

        # Wait a moment for the resource to be created
        sleep 3

        # Get the created job name using our unique label
        JOB_NAME=$(kubectl get rayjobs -n ai -l workflow-run="${RUN_LABEL}" -o jsonpath='{.items[0].metadata.name}')

        if [ -z "$JOB_NAME" ]; then
          echo "ERROR: Failed to retrieve RayJob name"
          kubectl get rayjobs -n ai -l workflow-run="${RUN_LABEL}"
          exit 1
        fi

        echo "‚úÖ Created RayJob: $JOB_NAME"
        echo "$JOB_NAME" > /tmp/job-name.txt

    # outputs:
    # parameters:
    # - name: job-name
    #   valueFrom:
    #     path: /tmp/job-name.txt

  # # Wait for RayJob
  # - name: wait-ray-job
  #   inputs:
  #     parameters:
  #     - name: job_name
  #   script:
  #     image: bitnami/kubectl:latest
  #     command: [sh]
  #     source: |
  #       echo "‚è≥ Waiting for RayJob {{inputs.parameters.job_name}}..."
  #       kubectl wait --for=condition=complete \
  #         --timeout=7200s \
  #         rayjob/{{inputs.parameters.job_name}} \
  #         -n ai
  #       echo "‚úÖ Training complete!"

  # # Compare and promote to staging
  # - name: compare-and-promote
  #   inputs:
  #     parameters:
  #     - name: model_name
  #     - name: metric
  #     - name: higher_is_better
  #     - name: threshold
  #     - name: mlflow_uri
  #   outputs:
  #     parameters:
  #     - name: ci_version
  #       valueFrom:
  #         path: /tmp/ci_version.txt
  #     - name: staging_version
  #       valueFrom:
  #         path: /tmp/staging_version.txt
  #   script:
  #     image: ghcr.io/mlflow/mlflow:v2.18.0
  #     command: [python]
  #     source: |
  #       from mlflow import MlflowClient
  #       import sys

  #       client = MlflowClient("{{inputs.parameters.mlflow_uri}}")
  #       model_name = "{{inputs.parameters.model_name}}"
  #       metric_name = "{{inputs.parameters.metric}}"
  #       higher_is_better = "{{inputs.parameters.higher_is_better}}" == "true"
  #       threshold = float("{{inputs.parameters.threshold}}")

  #       print(f"üîç Comparing {model_name}")
  #       print(f"   Metric: {metric_name} ({'higher' if higher_is_better else 'lower'} is better)")
  #       print(f"   Threshold: {threshold}")

  #       # Get latest CI version
  #       print(f"\nüìä Getting latest CI model...")
  #       ci_versions = client.search_model_versions(f"name='ci.{model_name}'")
  #       if not ci_versions:
  #           print(f"‚ùå No CI versions found for ci.{model_name}")
  #           sys.exit(1)

  #       latest_ci = sorted(ci_versions, key=lambda x: x.creation_timestamp, reverse=True)[0]
  #       ci_version = latest_ci.version
  #       print(f"   Latest CI: v{ci_version}")

  #       # Get CI metrics
  #       ci_run = client.get_run(latest_ci.run_id)
  #       if metric_name not in ci_run.data.metrics:
  #           print(f"‚ùå Metric '{metric_name}' not found in CI model")
  #           print(f"   Available: {list(ci_run.data.metrics.keys())}")
  #           sys.exit(1)

  #       ci_value = ci_run.data.metrics[metric_name]
  #       print(f"   CI {metric_name}: {ci_value}")

  #       # Try to get champion
  #       print(f"\nüèÜ Looking for champion...")
  #       should_promote = False
  #       try:
  #           champion = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
  #           print(f"   Found champion: v{champion.version}")

  #           champ_run = client.get_run(champion.run_id)
  #           if metric_name not in champ_run.data.metrics:
  #               print(f"   Champion missing {metric_name}, promoting CI")
  #               should_promote = True
  #           else:
  #               champ_value = champ_run.data.metrics[metric_name]
  #               print(f"   Champion {metric_name}: {champ_value}")

  #               # Compare
  #               if higher_is_better:
  #                   improvement = ci_value - champ_value
  #               else:
  #                   improvement = champ_value - ci_value

  #               print(f"   Improvement: {improvement:.6f}")

  #               if improvement >= threshold:
  #                   print(f"‚úÖ CI is better! ({improvement:.6f} >= {threshold:.6f})")
  #                   should_promote = True
  #               else:
  #                   print(f"‚ùå Not enough improvement ({improvement:.6f} < {threshold:.6f})")
  #                   sys.exit(1)
  #       except Exception as e:
  #           print(f"   No champion found: {e}")
  #           print("   Promoting first model!")
  #           should_promote = True

  #       if not should_promote:
  #           sys.exit(1)

  #       # Copy to staging
  #       print(f"\nüì¶ Copying ci.{model_name}/v{ci_version} ‚Üí staging.{model_name}")
  #       staging_mv = client.copy_model_version(
  #           f"models:/ci.{model_name}/{ci_version}",
  #           f"staging.{model_name}"
  #       )

  #       print(f"‚úÖ Created staging.{model_name}/v{staging_mv.version}")

  #       # Write outputs
  #       with open("/tmp/ci_version.txt", "w") as f:
  #           f.write(ci_version)
  #       with open("/tmp/staging_version.txt", "w") as f:
  #           f.write(staging_mv.version)

  #       print("‚úÖ Comparison complete!")

  # # Build staging image with BuildKit
  # - name: build-staging-image
  #   inputs:
  #     parameters:
  #     - name: model_name
  #     - name: staging_version
  #     - name: registry
  #     - name: mlflow_uri
  #     artifacts:
  #     - name: repo
  #       path: /work
  #       git:
  #         repo: https://github.com/opencloudhub/ai-ml-{{inputs.parameters.model_name}}.git
  #         revision: main
  #   outputs:
  #     parameters:
  #     - name: staging_image_tag
  #       valueFrom:
  #         path: /work/image_tag.txt
  #     - name: git_sha
  #       valueFrom:
  #         path: /work/git_sha.txt
  #   volumes:
  #   - name: docker-config
  #     secret:
  #       secretName: docker-secret
  #   container:
  #     image: moby/buildkit:v0.9.3-rootless
  #     readinessProbe:
  #       exec:
  #         command: [sh, -c, "buildctl debug workers"]
  #     volumeMounts:
  #     - name: work
  #       mountPath: /work
  #     - name: docker-config
  #       mountPath: /.docker
  #     workingDir: /work
  #     env:
  #     - name: BUILDKITD_FLAGS
  #       value: --oci-worker-no-process-sandbox
  #     - name: DOCKER_CONFIG
  #       value: /.docker
  #     command: [sh, -c]
  #     args:
  #     - |
  #       set -e

  #       # Get git SHA
  #       GIT_SHA=$(git rev-parse --short HEAD)
  #       IMAGE_TAG="{{inputs.parameters.registry}}/{{inputs.parameters.model_name}}-serving:model-v{{inputs.parameters.staging_version}}-${GIT_SHA}-staging"

  #       echo "$IMAGE_TAG" > image_tag.txt
  #       echo "$GIT_SHA" > git_sha.txt

  #       echo "üê≥ Building: $IMAGE_TAG"

  #       # Build with BuildKit
  #       buildctl-daemonless.sh build \
  #         --frontend dockerfile.v0 \
  #         --local context=/work \
  #         --local dockerfile=/work \
  #         --opt filename=Dockerfile.serving \
  #         --opt build-arg:MLFLOW_TRACKING_URI={{inputs.parameters.mlflow_uri}} \
  #         --opt build-arg:MODEL_NAME=staging.{{inputs.parameters.model_name}} \
  #         --opt build-arg:MODEL_VERSION={{inputs.parameters.staging_version}} \
  #         --output type=image,name=$IMAGE_TAG,push=true

  #       echo "‚úÖ Image built and pushed!"
  #   volumeClaimTemplates:
  #   - metadata:
  #       name: work
  #     spec:
  #       accessModes: ["ReadWriteOnce"]
  #       resources:
  #         requests:
  #           storage: 1Gi

  # # Test staging
  # - name: test-staging
  #   inputs:
  #     parameters:
  #     - name: image_tag
  #   script:
  #     image: alpine:latest
  #     command: [sh]
  #     source: |
  #       echo "üß™ Testing: {{inputs.parameters.image_tag}}"
  #       echo "‚úÖ Tests passed (dummy for POC)"

  # # Promote to production
  # - name: promote-to-production
  #   inputs:
  #     parameters:
  #     - name: model_name
  #     - name: staging_version
  #     - name: mlflow_uri
  #   outputs:
  #     parameters:
  #     - name: prod_version
  #       valueFrom:
  #         path: /tmp/prod_version.txt
  #   script:
  #     image: ghcr.io/mlflow/mlflow:v2.18.0
  #     command: [python]
  #     source: |
  #       from mlflow import MlflowClient

  #       client = MlflowClient("{{inputs.parameters.mlflow_uri}}")
  #       model_name = "{{inputs.parameters.model_name}}"
  #       staging_version = "{{inputs.parameters.staging_version}}"

  #       print(f"üöÄ Promoting to production: {model_name}")

  #       # Copy staging ‚Üí prod
  #       print(f"üì¶ Copying staging.{model_name}/v{staging_version} ‚Üí prod.{model_name}")
  #       prod_mv = client.copy_model_version(
  #           f"models:/staging.{model_name}/{staging_version}",
  #           f"prod.{model_name}"
  #       )

  #       print(f"‚úÖ Created prod.{model_name}/v{prod_mv.version}")

  #       # Update aliases
  #       try:
  #           old_champ = client.get_model_version_by_alias(f"prod.{model_name}", "champion")
  #           print(f"   Moving old champion v{old_champ.version} ‚Üí @previous")
  #           client.set_registered_model_alias(f"prod.{model_name}", "previous", old_champ.version)
  #       except:
  #           print("   No previous champion to archive")

  #       # Set new champion
  #       client.set_registered_model_alias(f"prod.{model_name}", "champion", prod_mv.version)
  #       print(f"‚úÖ Set prod.{model_name}@champion ‚Üí v{prod_mv.version}")

  #       with open("/tmp/prod_version.txt", "w") as f:
  #           f.write(prod_mv.version)

  #       print("‚úÖ Production promotion complete!")

  # # Retag image to production
  # - name: retag-image
  #   inputs:
  #     parameters:
  #     - name: staging_image
  #   outputs:
  #     parameters:
  #     - name: prod_image
  #       valueFrom:
  #         path: /tmp/prod_image.txt
  #   script:
  #     image: gcr.io/go-containerregistry/crane:latest
  #     command: [sh]
  #     source: |
  #       STAGING="{{inputs.parameters.staging_image}}"
  #       PROD="${STAGING%-staging}"

  #       echo "üè∑Ô∏è  Retagging image:"
  #       echo "   From: $STAGING"
  #       echo "   To:   $PROD"

  #       crane copy "$STAGING" "$PROD"

  #       echo "$PROD" > /tmp/prod_image.txt
  #       echo "‚úÖ Production image ready!"
