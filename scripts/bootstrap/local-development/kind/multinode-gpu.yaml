# Copyright 2024 NVIDIA CORPORATION.
# OpenCloudHub Multi-Node GPU Cluster Configuration for nvkind
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: opencloudhub-local-multinode-gpu
nodes:
  # Control plane node - Gateway API & Management
  - role: control-plane
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "networking.opencloudhub.org/gateway-controller=true,kubernetes.io/os=linux"
    extraPortMappings:
      # HTTP traffic
      - containerPort: 80
        hostPort: 8080
        protocol: TCP
      # HTTPS traffic
      - containerPort: 443
        hostPort: 8443
        protocol: TCP
      # Gateway API specific ports
      - containerPort: 15021
        hostPort: 15021
        protocol: TCP
      - containerPort: 15443
        hostPort: 15443
        protocol: TCP

  # Storage Node
  - role: worker
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "node.opencloudhub.org/type=storage,node.opencloudhub.org/storage=true"

  # Observability Node
  - role: worker
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "node.opencloudhub.org/type=observability,node.opencloudhub.org/observability=true"

  # Application + Inference Node (CPU-only)
  - role: worker
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "node.opencloudhub.org/type=application,node.opencloudhub.org/application=true"

{{- if gt numGPUs 0 }}
  # GPU Training Nodes - One worker per GPU for maximum isolation
{{- range $gpu := until numGPUs }}
  - role: worker
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "node.opencloudhub.org/type=gpu-training,node.opencloudhub.org/gpu-training=true,nvidia.com/gpu.present=true,nvidia.com/gpu.id={{ $gpu }}"
    extraMounts:
      # Inject specific GPU using nvidia-container-runtime
      # Requires `accept-nvidia-visible-devices-as-volume-mounts = true` in /etc/nvidia-container-runtime/config.toml
      - hostPath: /dev/null
        containerPath: /var/run/nvidia-container-devices/{{ $gpu }}
{{- end }}
{{- else }}
  # No GPUs detected - Create a single mock GPU training node for testing
  - role: worker
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "node.opencloudhub.org/type=gpu-training,node.opencloudhub.org/gpu-training=true,nvidia.com/gpu.mock=true"
{{- end }}
